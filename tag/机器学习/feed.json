{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hacking to the gate! • All posts by \"机器学习\" tag",
    "description": "Bipedal Bit's blog",
    "home_page_url": "https://blog.bipedalbit.net",
    "items": [
        {
            "id": "https://blog.bipedalbit.net/2015/12/05/%E7%94%B1%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E7%B1%BB%E8%84%91%E7%8E%B0%E7%8A%B6%E5%B1%95%E5%BC%80/",
            "url": "https://blog.bipedalbit.net/2015/12/05/%E7%94%B1%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E7%B1%BB%E8%84%91%E7%8E%B0%E7%8A%B6%E5%B1%95%E5%BC%80/",
            "title": "由人工神经网络和类脑现状展开",
            "date_published": "2015-12-05T14:07:40.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 今天听了个讲座，是一位鹅厂智库，是个博士，讲的关于互联网、人工智能与大脑的一些遐想。简单地说，就是认为互联网与大脑在形式上已经很相似了，可以借鉴脑神经科学来发展互联网，也可以用互联网的发展来反观大脑得到脑科学方面的启发。讲座期间提了一下人工智能，尤其是人工神经网络和类脑对大脑的仿生。讲座上我对ANN和类脑的技术现状有了一些思考，对互联网与脑神经科学的联系也有了一些自己的看法。</p>\n<span id=\"more\"></span>\n<h1 id=\"1-人工神经网络现状\"><a href=\"#1-人工神经网络现状\" class=\"headerlink\" title=\"1 人工神经网络现状\"></a>1 人工神经网络现状</h1><p>&nbsp;&nbsp;&nbsp; 之前<a href=\"http://blog.bipedalbit.net/2015/10/9/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/\">自科普BP神经网络</a>的时候提到过人工神经网络的仿生模型，看起来好像很有道理很靠谱，但是这个研究方向上出现过瓶颈。从前的单纯“神经网络”如BP，远远无法与人脑甚至小白鼠的大脑相提并论，几十几百个神经元，顶多也就模拟一下应激性或简单的反射行为，别说逻辑思考，就连动物本能的模拟都做不到。原因很简单，ANN的结构太简单了，真正的人的神经网络由上亿量级的神经元组成，其中包括了上百万个互相联系的与ANN同量级的反射组件。<br>&nbsp;&nbsp;&nbsp; 2006年之后，Deep Learning兴起，老实说我实在不太看得惯学界一些炒概念的行为，DL其实是提出了一些层数更多的ANN的可行解。比如CNN，在普通BP网络的前面加了若干曾卷积层，特化了BP的图像处理功能，做的事情类似于为简单反射添加了视网膜对图像的前期处理过程，可是由于BP的单一的简单的结构，仍然只能称作是复杂一点的反射模拟；又比如DBN，一节一节的增益、传递BP的前馈效应，节与节之间通过置信过程进行了信息的提纯，在我看来虽然真正有了模拟多层反射组件的趋势，结构仍然单一而简单，做的事情类似于手脚到大脑的长程简单反射行为。<br>&nbsp;&nbsp;&nbsp; 我不觉得DL真的突破了ANN的瓶颈。然而到了DL的程度，计算机的优势——硬件效率已经捉襟见肘。因为分布式集群难以达到能模拟上亿神经元或者说上百万反射组件<strong>并行交互</strong>的规模。这里就涉及了赛博世界的壁垒：宇宙中任何简单的过程都是在各种规则的限制下并行发生，各自独立进行的。而计算机一次却只能模拟一个简单物理过程，仅凭当前的技术再怎么并行化计算，并行量相对于一颗宇宙砂砾——生物的量级仍然嫌少。<br>&nbsp;&nbsp;&nbsp; 此外，即使假设我们已经拥有支持超大并发量的量子计算机，我们仍然需要组织更复杂的神经网络结构。在各种科幻作品中，AI之所以强大到令人类恐惧，是它们具备了自行学习，自组织自身结构的能力。也许你会说ANN不是已经能自行学习了吗？但是ANN乃至DL的学习结果被AI掌握了吗？不，连AI本身都还不存在，学习结果只能呈现给人类。那么这里我要说，AI更加重要，或许最重要的一项素质其实是结构自组织的能力。<br>&nbsp;&nbsp;&nbsp; 你可能会问，人类的脑神经似乎也没有怎么调整结构啊，怎么具有如此高的智能程度？我要驳斥两点，首先，现在看到的每一个人类个体的脑神经结构都是不是凭空产生的，亿万年的生物史中人类这条智慧生物演化线上调整自身的脑神经结构的过程从来没有停止过；其次，即使是一个固定人类个体，他的脑神经网络结构也是在进行自调整的，神经细胞的生长、萎缩、换代在人的一生中一直在进行，这不能单纯的映射为ANN中神经元间连接权重的调整，神经元的层次结构也在发生变化。<br>&nbsp;&nbsp;&nbsp; 谈及自适应的自结构调整我们要打住，我还得引入一下类脑的设想与技术现状。</p>\n<h1 id=\"2-类脑现状\"><a href=\"#2-类脑现状\" class=\"headerlink\" title=\"2 类脑现状\"></a>2 类脑现状</h1><p>&nbsp;&nbsp;&nbsp; 类脑是近两年炒起来的新概念，在我看来真正的类脑科技的前置科技还没有准备好，类脑这个概念来的太早了。我的根据有两点。<br>&nbsp;&nbsp;&nbsp; 首先，且不说具备自适应调整物理结构能力的类脑，即使要模拟一个静态的人脑物理结构，我们都很难做到，我们做不到那么大的并行量；我们没有量子计算机技术；我们能支撑的的分布式计算规模远不够大；我们不知道如何设计、组装各种反射部件（现在的ANN）成为一个成体系的具有综合思维能力和抽象逻辑能力的真正的神经网络。<br>&nbsp;&nbsp;&nbsp; 那么让人造物自适应的调整结构自己模拟一个优秀的脑神经网络呢？这是个好主意，通过现有经验的优化，人造物的演化速度可以比自然界中的生物快得多，且更有方向性。但是我们没有能够自行组合甚至仅仅是互相交互的纳米机器人技术，我们连会合体组合变形的机器人小组都都没有。<br>&nbsp;&nbsp;&nbsp; 我要这么说，要在硬件层面搞脑神经结构仿生，如果不抛弃纯机械处理单元，我们走不了多远。也许有读者注意到我隐约指出了硅基计算机之外的另一条技术道路。没错，除了量子计算机，我们还可以选择生物计算机，如果赛博世界无法创造出来，我们就用生化技术制造大量生物的处理单元完成巨量并行计算。<br>&nbsp;&nbsp;&nbsp; 更有甚者，我们可以直接把现有的哺乳动物个体乃至人类个体人道的接入高效通信的硅基网络，走人工蜂巢思维的路子。<br>&nbsp;&nbsp;&nbsp; 但不管怎么说，在我看来，现在所谓的带记忆的计算机计算部件的思路，实在是太保守也太天真了。<br>&nbsp;&nbsp;&nbsp; 但是要注意，我没有说非硬件的类脑不可实现，我们的专用分布式集群或许规模有限，但是互联网中终端的规模还是在不断扩大的。如果把互联网看做一个包含海量资源的物理世界，那么互联网中的每一个线程，都可以对应物理世界中的一个简单物理过程。一个程序虽然无法动态自适应的改变自身，但是如果把一个程序看成是一个生物个体呢？生物个体是可以死的，但是生物会在繁衍的过程中微调遗传因子，完成本种族的物理结构自调整，这种调整甚至可以是多版本、有分支的。是的，程序可以根据特定外部信息有方向的修改扩充自己的源码（毕竟程序一经编译就脱离与源码的联系了，就算是解释型语言的程序，执行完当前语句时，改动之前的语句也不会对程序的整体执行过程造成影响），然后命令编译新版本的源码，并运行新的可执行程序。<br>&nbsp;&nbsp;&nbsp; 简单而实际的说，我们可以把只会单纯复制自身的病毒程序，变成朝生夕死的蝼蚁。但是这些蝼蚁在繁衍（复制）的过程中可以根据我们赋予它们的经验迅速的、有方向的自适应微调自己的遗传因子（源码），甚至我们可以在每次繁衍（复制）的过程中让病毒程序“龙生九子各不同”。而外部信息，我们可以从互联网中获取，我们可以让病毒程序存活在每一台服务器，每一台终端机的冗余时空中。当然考虑到人类对未知和不可控事物的天然恐惧，我们也可以建立一个封闭的系统，但是我们仍然需要通过互联网获取信息不断更新（更新而不是扩充，因为独立系统空间有限）独立系统的知识库，让进化树在独立系统中自行生长。需要注意的是，如果我们指望病毒能迅速从蝼蚁变成具有更复杂结构的赛博生物，我们需要提醒病毒尽量与同类产生交互，这有助于共生生物群的产生。（毕竟我觉得每个人类个体其实也是一个巨大的共生生物群，其中的每个生物个体则是高度特化分工的细胞）<br>&nbsp;&nbsp;&nbsp; As far as I am concerned，赛博世界可以有人工智能，但是不要妄想只凭程序员自己，在<strong>一段源码</strong>中，封闭的完成AI的创造。智能个体必须是自适应调整的，不彻底放手让它演化，创造者将会成为它的进化瓶颈。</p>\n<h1 id=\"3-互联网与脑\"><a href=\"#3-互联网与脑\" class=\"headerlink\" title=\"3 互联网与脑\"></a>3 互联网与脑</h1><p>&nbsp;&nbsp;&nbsp; 这里我也顺便谈谈我对互联网与脑之间概念映射的看法。人们常常觉得互联网的基本单元应该是终端机或服务器，其实我觉得这是不对的，至少现在的互联网不应该这么看待。如果互联网是一个脑神经网络，那么互联网中的信息传递过程可以对应神经网络中的生物电信号传递，那互联网中的什么可以看成神经网络中的神经元呢？我觉得这一点上找不到物理的一一对应关系了，每一个被互联网连接起来的人类作为核心，供养人类的外部条件作为胶质细胞，人类使用的接入互联网的终端机设备作为突触，通信设备，包括光缆、交换机、路由器、服务器等等作为轴突和树突，这形成了一种互联网和神经网络间的扭曲的映射关系。<br>&nbsp;&nbsp;&nbsp; 值的注意的是，绝不应该把人类从互联网中割离开，离开了人类，互联网是死的。如果说互联网作为一个神经网络也指导行为，那么它是通过人类社会来行动的，人类社会是互联网特化分工的身体的一部分。互联网通过人类社会的行为，来优化调整自身的结构，来与物质世界交互。有读者可能会问，那么互联网就像一个生命了吧，它有意识吗？我们能跟它交流吗？我用一个问题来回答这种问题，你觉得我们人类要怎么跟自己的细胞来交流？<br>&nbsp;&nbsp;&nbsp; 这涉及一个有趣的观点，《左手疯子，右手天才》中提到过一种生命观，一个蚂蚁族群为什么要被看做许多生命个体？虽然蚂蚁之间被物理的隔离开，凭什么一个蚂蚁族群就不能看成是一个完整的生命体？蚂蚁不通过接触就不能连接在一个整体中了吗？为什么所有生命体都得跟人类似的，由一堆细胞生命抱成一团组成一个大的生命个体呢？<br>&nbsp;&nbsp;&nbsp; 我们可以这么看，如果细胞器是最小的生命体，那么细胞作为细胞器的社会，成了第二层的生命体，人类或其他多细胞生物就成了第三层的生命体，蚂蚁族群、人类社会或互联网难道不可以是第四层的生命体吗？</p>\n<p>&nbsp;&nbsp;&nbsp; 这是我第一次写这种信马由缰的非技术博文，只是我自己的小范围头脑风暴，有唐突冒失的地方读者多包含。</p>\n",
            "tags": [
                "机器学习",
                "人工神经网络",
                "类脑"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2015/10/13/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84python%EF%BC%88pybrain%E5%BA%93%EF%BC%89%E5%AE%9E%E7%8E%B0/",
            "url": "https://blog.bipedalbit.net/2015/10/13/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84python%EF%BC%88pybrain%E5%BA%93%EF%BC%89%E5%AE%9E%E7%8E%B0/",
            "title": "BP神经网络的python（pybrain库）实现",
            "date_published": "2015-10-13T06:54:25.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 上一篇文用C++手写了BP神经网络，事实上，生产环境中已经很少有人再亲自写神经网络了，已经出现了很多成熟的机器学习开源库。我安装了C++的shark库和python的pybrain库，在研究shark库的文档时发现shark库封装度太低了，用起来很麻烦，遂弃。虽然放弃C++的高效库很遗憾，但是后来发现pybrain库真是非常友好非常方便，于是分别写了分类器和函数拟合的例子及测评。</p>\n<span id=\"more\"></span>\n<p>&nbsp;&nbsp;&nbsp; 看两个库的源码时发现，shark库的机器学习模型比较多，pybrain的少一些，但是提供一些实时图形化的接口（虽然我嫌麻烦没用）。于是pybrain也可以与matlab和R语言争争份额了（虽然显然还是matlab和R比较专业）。<br>&nbsp;&nbsp;&nbsp; 首先来看分类器的实现，还是那个寻找点与平面位置关系规则的问题，上代码，NNClassifier.py：</p>\n<pre><code class=\"hljs\">#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# 关于前馈神经网络二分类点集的尝试\n# 因为是首次尝试使用pybrain，不使用快捷方式创建神经网络以求思路清晰。\n\n__author__ = &#39;Bipedal Bit, hmemoryl@163.com&#39;\n\nfrom pybrain.structure import FeedForwardNetwork\nfrom pybrain.structure import LinearLayer, SigmoidLayer\nfrom pybrain.structure import FullConnection\nfrom pybrain.datasets import SupervisedDataSet\nimport random\nfrom pybrain.supervised.trainers import BackpropTrainer\nimport time\nfrom pybrain.tools.customxml.networkwriter import NetworkWriter\nfrom pybrain.tools.customxml.networkreader import NetworkReader\n\n# ================== 构造神经网络 ======================\n# 建立前馈神经网络FNN\nFNN = FeedForwardNetwork()\n\n# 逐层建立神经元\n# 建立m个输入参数的输入层（inLayer）\ninLayer = LinearLayer(7, name = &#39;inLayer&#39;)\n# 建立隐含层\nhiddenLayer0 = SigmoidLayer(30, name = &#39;hiddenLayer0&#39;)\n# 建立n个输出参数的输出层（outLayer）\noutLayer = SigmoidLayer(2, name = &#39;outLayer&#39;)\n\n# 将层加入神经网络（向神经网络内添加不同功能的神经元）\nFNN.addInputModule(inLayer)\nFNN.addModule(hiddenLayer0)\nFNN.addOutputModule(outLayer)\n\n# 建立层之间的连接，使用全连接，即两层的神经元之间两两全部连接\nconn_inToHidden0 = FullConnection(inLayer, hiddenLayer0)\nconn_hidden0ToOut = FullConnection(hiddenLayer0, outLayer)\n\n# 将连接加入神经网络\nFNN.addConnection(conn_inToHidden0)\nFNN.addConnection(conn_hidden0ToOut)\n\n# 神经元拓扑排序，准备使用\nFNN.sortModules()\n\n# ================== 构造数据集 ======================\n# 建立带监督的数据集，参数为输入维数和输出维数\nDS = SupervisedDataSet(7, 2)\n\n# 向数据集添加样本\n# 将一个0~1的随机数扩展到适当的范围\ndef expand(n):\n\treturn n*20 - 10\n# 数据组数\ndataCnt = 2000\nfor i in xrange(1, dataCnt):\n\t# 生成归一化样本数据\n\t_x = random.random()\n\t_y = random.random()\n\t_z = random.random()\n\t_a = random.random()\n\t_b = random.random()\n\t_c = random.random()\n\t_d = random.random()\n\t# 扩散出样本真实值\n\tx = expand(_x)\n\ty = expand(_y)\n\tz = expand(_z)\n\ta = expand(_a)\n\tb = expand(_b)\n\tc = expand(_c)\n\td = expand(_d)\n\t# 计算监督值\n\tabove = 1 if z &gt; (a*x + b*y + d)/(-c) else 0\n\tbelow = 1 if above == 0 else 0\n\t# 添加数据\n\tDS.addSample([_x, _y, _z, _a, _b, _c, _d], [above, below])\n\n# 分别索引输入/输出数据集合\nX, Y = DS[&#39;input&#39;], DS[&#39;target&#39;]\n\n# 按比例切分训练集与测试集（训练集：测试集 = 8:2）\nDSTrain, DSTest = DS.splitWithProportion(0.8)\n\n# 分别索引训练集和测试集的输入/输出数据集合\nXTrain, YTrain = DSTrain[&#39;input&#39;], DSTrain[&#39;target&#39;]\nXTest, YTest = DSTest[&#39;input&#39;], DSTest[&#39;target&#39;]\n\n# =================== 读取神经网络 =====================\nFNN = NetworkReader.readFrom(&#39;networkClasssifier.xml&#39;)\n\n# ================== 训练神经网络 ======================\n# 使用BP算法训练神经网络\ntrainer = BackpropTrainer(FNN, DSTrain, verbose = True, learningrate = 0.01, lrdecay = 1, momentum = 0)\n\n# 开始计时\nt_start = time.clock()\n\n# 训练至收敛\ntrainer.trainUntilConvergence(maxEpochs = 100)\n\n# 计时结束\nt_end = time.clock()\n\n# 输出训练用时\nprint &#39;训练用时：&#39;, t_end - t_start, &#39;s&#39;\n\n# =================== 存储神经网络 =====================\nNetworkWriter.writeToFile(FNN, &#39;networkClasssifier.xml&#39;)\n\n# =================== 结果可视化 =======================\n\n\n# ====================== 测试 ==========================\nsuccessCnt = 0\nfor i in xrange(0, len(XTest)-1):\n\t# 获取测试数据\n\tx = XTest[i]\n\ty = YTest[i]\n\t# 测试\n\tprediction = FNN.activate(x)\n\tif (y[0] - y[1])*(prediction[0] - prediction[1]) &gt; 0:\n\t\tsuccessCnt += 1\nprint &#39;测试：&#39;, len(XTest)\nprint &#39;命中：&#39;, successCnt\nprint &#39;命中率：&#39;, (successCnt*1.)/len(XTest)*100, &#39;%&#39;\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 训练结果我不太意外，误差函数值在0.05左右，算不上好，正确率在82%左右，只能说还行。这次只是简单测评，就不对收敛程度做太高要求了。（其实是我把隐含层神经元都加到30个了精度也不是很高，有点累了，毕竟经验公式说这种输入输出参数量最多加到13个）<br>&nbsp;&nbsp;&nbsp; 最后的神经网络保存文档（权重数组），networkClassifier.xml：</p>\n<pre><code class=\"hljs\">&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;PyBrain&gt;\n\t&lt;Network class=&quot;pybrain.structure.networks.feedforward.FeedForwardNetwork&quot; name=&quot;FeedForwardNetwork-5&quot;&gt;\n\t\t&lt;name val=&quot;u&#39;FeedForwardNetwork-5&#39;&quot;/&gt;\n\t\t&lt;Modules&gt;\n\t\t\t&lt;LinearLayer class=&quot;pybrain.structure.modules.linearlayer.LinearLayer&quot; inmodule=&quot;True&quot; name=&quot;inLayer&quot;&gt;\n\t\t\t\t&lt;dim val=&quot;7&quot;/&gt;\n\t\t\t\t&lt;name val=&quot;&#39;inLayer&#39;&quot;/&gt;\n\t\t\t&lt;/LinearLayer&gt;\n\t\t\t&lt;SigmoidLayer class=&quot;pybrain.structure.modules.sigmoidlayer.SigmoidLayer&quot; name=&quot;outLayer&quot; outmodule=&quot;True&quot;&gt;\n\t\t\t\t&lt;dim val=&quot;2&quot;/&gt;\n\t\t\t\t&lt;name val=&quot;&#39;outLayer&#39;&quot;/&gt;\n\t\t\t&lt;/SigmoidLayer&gt;\n\t\t\t&lt;SigmoidLayer class=&quot;pybrain.structure.modules.sigmoidlayer.SigmoidLayer&quot; name=&quot;hiddenLayer0&quot;&gt;\n\t\t\t\t&lt;dim val=&quot;30&quot;/&gt;\n\t\t\t\t&lt;name val=&quot;&#39;hiddenLayer0&#39;&quot;/&gt;\n\t\t\t&lt;/SigmoidLayer&gt;\n\t\t&lt;/Modules&gt;\n\t\t&lt;Connections&gt;\n\t\t\t&lt;FullConnection class=&quot;pybrain.structure.connections.full.FullConnection&quot; name=&quot;FullConnection-3&quot;&gt;\n\t\t\t\t&lt;inmod val=&quot;inLayer&quot;/&gt;\n\t\t\t\t&lt;outmod val=&quot;hiddenLayer0&quot;/&gt;\n\t\t\t\t&lt;Parameters&gt;[-4.4634646760183774, -1.8663551824786604, 1.5796984784944188, -4.1545734140399917, -1.4235379909335284, 10.814418886792573, -0.35697065520281313, 3.7168599840362613, -1.3954006078608174, 3.2888708257680479, -1.2728713382523704, 5.1682485849770519, -10.345459393449577, 0.40944729400489821, -0.86432327459917391, 0.87415267739043501, -1.1261847464737353, 0.26550824904566483, -0.69655632472406404, -1.0926496454783619, -1.3241014363212174, -1.2345262671004642, -5.3545154953074556, -3.2977309106602868, -1.9192142648212187, 3.5219032402205235, 8.8243116475870895, -0.34711073857964059, -0.76652402450096546, -0.45800625182882904, 0.10921234310650819, 1.0906882881030282, 0.58122385851806679, 0.74902769546891046, 0.57778560396382117, -0.36268964602939496, -1.4560716552974731, 0.025807475393432977, -0.52324632490359346, -1.4768259974689912, -1.3226548504200857, -0.56024431774602601, 0.5253813385807985, -2.3601368528359932, 3.4391924327008545, -3.4529059457340887, -0.36798304125763204, 0.24642387811765787, 0.74392363594535504, -1.4200092817628436, -0.95882940181717424, 0.90204884683974906, -2.2123307425066581, -1.2927495594815508, -2.6418298086483243, 0.0089737728359628229, -0.96131150512314989, -0.42386913703414159, 2.7423827971590629, 0.49257080970361217, -0.73140003600797654, -0.97237673817617065, 0.021672107067701795, -0.30730995794768651, -0.28377110259164756, -1.3769421502868895, -0.49923480994397734, 0.63167130189837051, 0.72750574299622528, -0.50823243724360356, -2.3249236293984685, -1.0084930398088603, 0.55483423319351144, -3.0895951468032683, -1.569810215716863, -4.6712778120909002, 2.1476977927094589, -0.92601922552049853, 0.80913122603871213, 1.0167819396610751, 1.2578916110729925, 1.1336741124853456, -1.4462571148912973, 0.4498637409091461, -0.0046570255381198191, 1.0130490743040022, 0.067808742574706565, -3.1665123334368257, -1.3695841902759027, -0.61944129404109394, 0.32712520616674406, -4.0944887153367588, 0.33425112596763401, 0.53786026328450398, 3.783707069693385, -0.73771116333542275, -7.3116843863108363, -0.90847319043398045, -1.3072997563815252, 1.5864357535576585, -2.1773369395271285, 0.16582713303194607, 0.62423604397793453, 0.59389298905810417, -0.31484499272512129, 5.0708486445028633, 0.91681404432938907, 0.31193631582711534, -5.4473204154043851, -0.7847240929315491, -7.6951030649320984, -0.32014838444510901, 0.56829986867023885, -0.29653458065896326, 0.51800388373753137, -0.56704382593349767, -0.25264606109631066, 0.78514270624403915, 0.67787739064705022, -0.0014430992506962728, -1.1915649958214867, -2.8339864138165369, -1.2920958546780335, -1.2004239162539645, -5.5518360974893373, 2.6887049948956299, -1.8837019633804577, 2.8587109449831924, 3.8684384835516568, -2.7090025688862394, -3.3154340336962216, 1.0688334874085363, 0.43275994294040698, -3.8784776661421345, -0.8578678330233066, 1.8270738585600814, 5.4477321487215189, 2.7554123218891138, -6.9965304625370077, 0.91986888975642622, 1.6206020126465939, 0.40565735648759632, 1.6264383151366206, -2.0910948725615417, -0.96054739666906863, 0.83967430154853273, 0.17826444997023716, 0.93718547827806598, -5.2232302716871377, -0.37715953425266679, 1.1057273981735851, -5.0228630028488084, 8.7833592718428406, -0.17862770462286087, 0.071119534208408983, 0.61781354218205642, -2.0482041559354802, -1.2286398440013546, 0.30582092080786355, 0.74602342033773583, -0.12623275590092933, -0.89369108883986548, -1.0173825208087091, 0.21764568858325722, 1.1182634120634272, 0.44364418276133, 1.138196191040812, 0.10662608395776786, -0.25838212820199019, -0.86982182253008533, -0.55680951312883542, 0.25307613718779792, -0.90155685220035831, 1.835084251778919, -0.81174579278015824, 0.48215346567645934, 6.267187930149162, -2.1805487376071957, 1.0628647712248505, -3.0943161262836401, -4.9327587898336835, 1.3749005504013798, -5.5102212316356844, -0.96863134222439218, 0.8268862262896568, 4.3540259268851402, -0.50518156739373588, 3.8934730549155523, -0.39568193061006912, -3.7859120847720105, 2.0595665943684325, -2.6633982082854342, -4.0807360028025208, 3.7442177075679623, 5.11773145948146, -0.44630616509537863, 0.20714091867709217, -0.6873175410894995, 0.061460095180831932, -0.44453400594106057, 0.35250696385984698, 2.2565455035241082, -0.079401700376001166, -0.18177974449629683, 0.71736053566879954, 0.54270386006022042, 0.75951868740937811, 1.3078378324277897, -2.1881100701125251, 0.84052630355979829]&lt;/Parameters&gt;\n\t\t\t&lt;/FullConnection&gt;\n\t\t\t&lt;FullConnection class=&quot;pybrain.structure.connections.full.FullConnection&quot; name=&quot;FullConnection-4&quot;&gt;\n\t\t\t\t&lt;inmod val=&quot;hiddenLayer0&quot;/&gt;\n\t\t\t\t&lt;outmod val=&quot;outLayer&quot;/&gt;\n\t\t\t\t&lt;Parameters&gt;[-8.2129041568200112, -6.7995955920941888, 0.48890334859207907, 7.6066781773027499, 2.8270407553317769, -1.0008489151414828, 3.0303929667267862, -2.4507639356974593, 1.9205137372795547, -2.0677111079872867, -4.6278733853833467, 3.3159637261922645, 0.4984027688395733, 4.7415120162351183, -2.5033978652108386, 5.1877234768369922, 0.87623718221836022, -5.0415283550332814, 4.0707627481245554, -6.4411441580882105, 2.6369969676415286, -8.5775684178164635, -1.313121167107631, -0.89764250697180914, -0.78729432668282306, -4.1300664676586489, 5.6446245751676667, -4.5102393183383498, 0.1106246177522491, 3.2716628012569964, 8.2844165753556958, 6.9238002822405154, 0.33656468267775197, -7.6960641823743456, 0.84148765790877422, 1.7483968344478173, -2.9938885121262571, 2.2508496743698845, -3.3237973590853258, 0.25502346696281558, 4.4894675338875496, -2.5350467550996507, -0.11269663125646072, -4.7593000820253231, 1.8099587344845871, -5.1168105240606856, -1.7100116403752497, 4.8179029808866174, -4.1421556373685728, 6.5870613347344591, -1.9261388149076637, 8.4927720262891935, 1.6203591174629115, -1.702384877536026, 1.9822996827269457, 3.6477517464649072, -5.4008040115101963, 4.720080872228464, -0.76546727507485091, -3.6703618616272795]&lt;/Parameters&gt;\n\t\t\t&lt;/FullConnection&gt;\n\t\t&lt;/Connections&gt;\n\t&lt;/Network&gt;\n&lt;/PyBrain&gt;\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 函数拟合实验选择了简单的a+b问题，上代码，NNRegression.py：</p>\n<pre><code class=\"hljs\">#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# 关于前馈神经网络函数拟合的尝试\n# 因为是首次尝试使用pybrain，不使用快捷方式创建神经网络以求思路清晰。\n\n__author__ = &#39;Bipedal Bit, hmemoryl@163.com&#39;\n\nfrom pybrain.structure import FeedForwardNetwork\nfrom pybrain.structure import LinearLayer, SigmoidLayer\nfrom pybrain.structure import FullConnection\nfrom pybrain.datasets import SupervisedDataSet\nimport random\nfrom pybrain.supervised.trainers import BackpropTrainer\nimport time\nfrom pybrain.tools.customxml.networkwriter import NetworkWriter\nfrom pybrain.tools.customxml.networkreader import NetworkReader\n\n# ================== 构造神经网络 ======================\n# 建立前馈神经网络FNN\nFNN = FeedForwardNetwork()\n\n# 逐层建立神经元\n# 建立m个输入参数的输入层（inLayer）\ninLayer = LinearLayer(2, name = &#39;inLayer&#39;)\n# 建立隐含层\nhiddenLayer0 = SigmoidLayer(20, name = &#39;hiddenLayer0&#39;)\n# 建立n个输出参数的输出层（outLayer）\noutLayer = LinearLayer(1, name = &#39;outLayer&#39;)\n\n# 将层加入神经网络（向神经网络内添加不同功能的神经元）\nFNN.addInputModule(inLayer)\nFNN.addModule(hiddenLayer0)\nFNN.addOutputModule(outLayer)\n\n# 建立层之间的连接，使用全连接，即两层的神经元之间两两全部连接\nconn_inToHidden0 = FullConnection(inLayer, hiddenLayer0)\nconn_hidden0ToOut = FullConnection(hiddenLayer0, outLayer)\n\n# 将连接加入神经网络\nFNN.addConnection(conn_inToHidden0)\nFNN.addConnection(conn_hidden0ToOut)\n\n# 神经元拓扑排序，准备使用\nFNN.sortModules()\n\n# ================== 构造数据集 ======================\n# 建立带监督的数据集，参数为输入维数和输出维数\nDS = SupervisedDataSet(2, 1)\n\n# 向数据集添加样本\n# 数据组数\ndataCnt = 1000\nfor i in xrange(1, dataCnt):\n\t# 生成归一化样本数据\n\ta = random.random()\n\tb = random.random()\n\t# 计算监督值\n\ts = a*20 - 10 + b*20 - 10\n\t# 添加数据\n\tDS.addSample([a, b], [s])\n\n# 分别索引输入/输出数据集合\nX, Y = DS[&#39;input&#39;], DS[&#39;target&#39;]\n\n# 按比例切分训练集与测试集（训练集：测试集 = 8:2）\nDSTrain, DSTest = DS.splitWithProportion(0.8)\n\n# 分别索引训练集和测试集的输入/输出数据集合\nXTrain, YTrain = DSTrain[&#39;input&#39;], DSTrain[&#39;target&#39;]\nXTest, YTest = DSTest[&#39;input&#39;], DSTest[&#39;target&#39;]\n\n# =================== 读取神经网络 =====================\nFNN = NetworkReader.readFrom(&#39;networkRegression.xml&#39;)\n\n# ================== 训练神经网络 ======================\n# 使用BP算法训练神经网络\ntrainer = BackpropTrainer(FNN, DSTrain, verbose = True, learningrate = 0.01, lrdecay = 1, momentum = 0)\n\n# 开始计时\nt_start = time.clock()\n\n# 训练至收敛\ntrainer.trainUntilConvergence(maxEpochs = 100)\n\n# 计时结束\nt_end = time.clock()\n\n# 输出训练用时\nprint &#39;训练用时：&#39;, t_end - t_start, &#39;s&#39;\n\n# =================== 存储神经网络 =====================\nNetworkWriter.writeToFile(FNN, &#39;networkRegression.xml&#39;)\n\n# =================== 结果可视化 =======================\n\n\n# ====================== 测试 ==========================\nfor i in xrange(0, 10):#len(XTest)-1):\n\t# 获取测试数据\n\tx = XTest[i]\n\ty = YTest[i]\n\t# 测试\n\tprediction = FNN.activate(x)\n\tprint &#39;期望值：&#39;, y\n\tprint &#39;预测值：&#39;, prediction\n\tprint &#39;--------------------------------------&#39;\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 评测结果有些诡异，尽管我丧心病狂的把隐含层神经元数慢慢加到了20个之多，收敛精度依然不太高。误差函数值在0.016左右，计算结果多的时候能有一点几的误差。至于为什么诡异，你们知道我用C++手写的那个BP神经网络a+b拟合测评结果什么样吗？我每次增加隐含层神经元，精度都在提高，最后加到11个神经元时，误差值只有10^-7的数量级了。我在想，莫非pybrain的BP实现写的比我还糟糕？当然我没有闲到对比源码找差别的地步，毕竟这是在扩展知识面而不是开题写论文，要学的东西还有很多。<br>&nbsp;&nbsp;&nbsp; 训练结果，networkRegression.xml：</p>\n<pre><code class=\"hljs\">&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;PyBrain&gt;\n\t&lt;Network class=&quot;pybrain.structure.networks.feedforward.FeedForwardNetwork&quot; name=&quot;FeedForwardNetwork-5&quot;&gt;\n\t\t&lt;name val=&quot;u&#39;FeedForwardNetwork-5&#39;&quot;/&gt;\n\t\t&lt;Modules&gt;\n\t\t\t&lt;LinearLayer class=&quot;pybrain.structure.modules.linearlayer.LinearLayer&quot; inmodule=&quot;True&quot; name=&quot;inLayer&quot;&gt;\n\t\t\t\t&lt;dim val=&quot;2&quot;/&gt;\n\t\t\t\t&lt;name val=&quot;&#39;inLayer&#39;&quot;/&gt;\n\t\t\t&lt;/LinearLayer&gt;\n\t\t\t&lt;LinearLayer class=&quot;pybrain.structure.modules.linearlayer.LinearLayer&quot; name=&quot;outLayer&quot; outmodule=&quot;True&quot;&gt;\n\t\t\t\t&lt;dim val=&quot;1&quot;/&gt;\n\t\t\t\t&lt;name val=&quot;&#39;outLayer&#39;&quot;/&gt;\n\t\t\t&lt;/LinearLayer&gt;\n\t\t\t&lt;SigmoidLayer class=&quot;pybrain.structure.modules.sigmoidlayer.SigmoidLayer&quot; name=&quot;hiddenLayer0&quot;&gt;\n\t\t\t\t&lt;dim val=&quot;20&quot;/&gt;\n\t\t\t\t&lt;name val=&quot;&#39;hiddenLayer0&#39;&quot;/&gt;\n\t\t\t&lt;/SigmoidLayer&gt;\n\t\t&lt;/Modules&gt;\n\t\t&lt;Connections&gt;\n\t\t\t&lt;FullConnection class=&quot;pybrain.structure.connections.full.FullConnection&quot; name=&quot;FullConnection-4&quot;&gt;\n\t\t\t\t&lt;inmod val=&quot;inLayer&quot;/&gt;\n\t\t\t\t&lt;outmod val=&quot;hiddenLayer0&quot;/&gt;\n\t\t\t\t&lt;Parameters&gt;[0.66989695089493184, 0.20154541787085151, -0.50128908433773467, -0.37527744480307418, -0.74611014578997958, -0.12337190284043228, -0.26139108724410959, -0.62223790666112244, 0.76934737325151314, 0.097273365366757986, -0.70241911419677627, -0.1675521617762078, -0.52875791799833338, -0.34701820533745775, 0.91136351597023102, -0.051368168009540249, -0.46415381076169848, -0.41344842847904884, -0.3637989937993043, -0.51709189928560939, 0.021917540062381426, -0.91373858562576571, 0.18558709721271155, 0.70146289478301882, 0.1657642703601043, 0.72042804496432955, -0.051995836002024759, -0.83682492233986472, -0.57633470846976453, -0.29793296256418189, -0.0042668700361729222, 0.89613628191472339, 0.78248084199047585, 0.083752407601260875, -0.34426021947865065, -0.53677318854588607, -0.59630570148683881, -0.2772033711210245, -0.29072335479904238, -0.59221753051512371]&lt;/Parameters&gt;\n\t\t\t&lt;/FullConnection&gt;\n\t\t\t&lt;FullConnection class=&quot;pybrain.structure.connections.full.FullConnection&quot; name=&quot;FullConnection-3&quot;&gt;\n\t\t\t\t&lt;inmod val=&quot;hiddenLayer0&quot;/&gt;\n\t\t\t\t&lt;outmod val=&quot;outLayer&quot;/&gt;\n\t\t\t\t&lt;Parameters&gt;[7.0033787939978653, -8.9204704009603475, -7.0019357487260141, -9.1851348838825597, 11.222280489358978, -9.1321512616209173, -10.187840330915042, 12.40878268578923, -8.1946287348945397, -9.729867458003703, -10.577209749285847, 12.709727310110253, 9.5182614987221701, -8.4580456500212975, -7.9531433608618309, 11.918092172946009, 10.86011334049692, -8.310811818166254, -9.5385851767871586, -9.531838664642386]&lt;/Parameters&gt;\n\t\t\t&lt;/FullConnection&gt;\n\t\t&lt;/Connections&gt;\n\t&lt;/Network&gt;\n&lt;/PyBrain&gt;\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 以上。欢迎留言探讨。</p>\n",
            "tags": [
                "BP神经网络",
                "机器学习",
                "python",
                "pybrain"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2015/10/11/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84C-%E5%AE%9E%E7%8E%B0/",
            "url": "https://blog.bipedalbit.net/2015/10/11/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84C-%E5%AE%9E%E7%8E%B0/",
            "title": "BP神经网络的C++实现",
            "date_published": "2015-10-10T16:28:00.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 之前说了组里的任务是手写BP神经网络，上一篇文总结了一下BP神经网络的概念，老实说，总结概念前的一个C++实现版本在总结概念之后重新审视时觉得实在是惨不忍睹，于是今晚回炉重写了。这篇文就来挂我的BP神经网络C++实现。</p>\n<span id=\"more\"></span>\n\n<p>&nbsp;&nbsp;&nbsp; 老师提出的具体问题是平面对点集的二分类。ACM战过这么多场，写板子早成了习惯，就把通用的BP认真封装了一下：<br>&nbsp;&nbsp;&nbsp; 头文件里的Data结构体是输入数据的数据结构，可自定义，这里用的是点分类问题的模型，BP.h：</p>\n<pre><code class=\"hljs\">#ifndef _BP_H_\n#define _BP_H_\n\n#include &lt;vector&gt;\n#include &lt;string&gt;\nusing namespace std;\n\n/* 数据样本类 */\nstruct Data\n&#123;\n\t/* 输入参数，包括： */\n\t/* 三维直角坐标系点的坐标(x, y, z) */\n\t/* 三维直角坐标系平面a*x + b*y + c*z + d = 0的4个系数 */\n\tdouble x[7];\n\t/* 期望输出即监督值 */\n\t/* 拟合 */\n\t// double d[1];\n\t/* 分类 */\n\tdouble d[2];\n\t/* 数据构造函数 */\n\tData();\n&#125;;\n\nclass BP\n&#123;\nprivate:\n\t/* ========== 常数 ========== */\n\t/* 输入层节点数 */\n\tint I;\n\t/* 隐含层神经元数 */\n\tint H;\n\t/* 输出层神经元数 */\n\tint O;\n\t/* 权重学习速率 */\n\tdouble LR;\n\t/* 偏置学习速率 */\n\tdouble LR2;\n\t/* 学习速率衰减率（每次衰减与当前LR相乘） */\n\tdouble LRDecay;\n\t/* 误差函数收敛阈值 */\n\tdouble C;\n\t/* ========== 容器 ========== */\n\t/* 训练用数据样本集 */\n\tvector&lt;Data&gt; trainDS;\n\t/* 测使用数据集 */\n\tvector&lt;Data&gt; testDS;\n\t/* 输入层与隐含层间的全连接权重：w[I(包含一个偏置值)][H] */\n\tdouble **w;\n\t/* w修正值 */\n\tdouble *dw;\n\t/* 隐含层阈值 */\n\tdouble *th;\n\t/* 隐含层输入积累即净激活，也存放之后的激活输出值：u[H] */\n\tdouble *u;\n\t/* 隐含层与输出层间的全连接权重：v[H][O] */\n\tdouble **v;\n\t/* v修正值 */\n\tdouble *dv;\n\t/* 输出层阈值 */\n\tdouble *to;\n\t/* 输出层输入积累即净激活，也存放之后的激活输出值：y[O] */\n\tdouble *y;\n\t/* 拟合标识 */\n\tbool regression;\n\t/* ========== 方法 ========== */\n\t/* 填充训练用数据样本集 */\n\tvoid fillTrainDS(int sampleCnt);\n\t/* 清空训练用数据样本集 */\n\tvoid clearTrainDS();\n\t/* 填充测试使用数据集 */\n\tvoid fillTestDS(int sampleCnt);\n\t/* 清空测试使用数据集 */\n\tvoid clearTestDS();\n\t/* 预测 */\n\tvoid forward(int index, bool test = false);\n\t/* 调整 */\n\tvoid backward(int index);\npublic:\n\t/* ========== 接口 ========== */\n\t/*\n\t * 类构造函数，初始化BP神经网络结构和训练参数\n\t * int _I：\t\t输入参数数目。\n\t * int _O：\t\t输出值数目。\n\t * int A：\t\t隐含层调整因子（1~10）。\n\t * double _LR：\t\t权重学习速率（0.01~0.8）。\n\t * double _LR2：\t偏置学习速率（0.01~0.8）。\n\t * double _LRDecay：\t学习速率衰减率（每次衰减与当前LR相乘）。\n\t * double _C：\t\t误差函数收敛阈值。\n\t * bool regression：\t拟合标识。\n\t */\n\tBP(int _I, int _O, int A = 1, double _LR = 0.01, double _LR2 = 0.035, double _LRDecay = 1.0, double _C = 0.01, bool regression = false);\n\t/* 类析构函数，释放容器分配的堆空间 */\n\t~BP();\n\t/* 使用指定数目的样本训练指定数目次循环，返回最后的误差函数值 */\n\tdouble train(int sampleCnt = 1000, int trainCnt = 100);\n\t/*\n\t * 使用指定数目的样本循环训练。\n\t * 误差函数值进入可接受范围判定收敛并停止训练；\n\t * 到达最大训练次数时停止训练。\n\t * 返回是否收敛。\n\t */\n\tbool trainTillConvergent(int sampleCnt = 1000, int maxEpoch = 1000);\n\t/* 生成指定数目组数据测试当前神经网络 */\n\tvoid testNetwork(int testCnt = 1000);\n\t/* 保存当前神经网络，即两个权重数组 */\n\tvoid saveNetwork(string wPath = &quot;wNetwork&quot;, string vPath = &quot;vNetwork&quot;);\n\t/* 载入两个权重数组，还原神经网络 */\n\tvoid loadNetwork(string wPath = &quot;wNetwork&quot;, string vPath = &quot;vNetwork&quot;);\n\t/* 使用指定数据预测输出值并返回输出值 */\n\tvector&lt;double&gt; runNetwork(vector&lt;double&gt; x);\n&#125;;\n\n#endif\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 类实现源码里的Data结构体的构造函数是输入数据的处理和问题模型的建立，可自定义，这里用的是点分类问题的数据处理，BP.cpp：</p>\n<pre><code class=\"hljs\">#include &quot;BP.h&quot;\n#include &lt;iostream&gt;\n#include &lt;cstdlib&gt;\n#include &lt;ctime&gt;\n#include &lt;cmath&gt;\n#include &lt;fstream&gt;\nusing namespace std;\n\n/* 数据构造函数 */\nData::Data()\n&#123;\n\t/* 归一参数 */\n\tfor(int i = 1 ; i &lt; 8 ; i++)\n\t&#123;\n\t\tx[i] = rand()/(double)RAND_MAX;\n\t&#125;\n\t/* 实际参数 */\n\tdouble _x = x[1]*20 - 10;\n\tdouble y = x[2]*20 - 10;\n\tdouble z = x[3]*20 - 10;\n\tdouble a = x[4]*20 - 10;\n\tdouble b = x[5]*20 - 10;\n\tdouble c = x[6]*20 - 10;\n\tdouble _d = x[7]*20 - 10;\n\t/* 拟合 */\n\t// d[0] = z - (a*_x + b*y + _d)/(-1*c);\n\t/* 分类 */\n\td[0] = z &gt; (a*_x + b*y + _d)/(-1*c) ? 1 : 0;\n\td[1] = d[0] ? 0 : 1;\n&#125;\n\n/* 填充训练用数据样本集 */\nvoid BP::fillTrainDS(int sampleCnt)\n&#123;\n\twhile(sampleCnt--)\n\t&#123;\n\t\ttrainDS.push_back(Data());\n\t&#125;\n&#125;\n\n/* 清空训练用数据样本集 */\nvoid BP::clearTrainDS()\n&#123;\n\tvector&lt;Data&gt; v;\n\ttrainDS.swap(v);\n&#125;\n\n/* 填充测试使用数据集 */\nvoid BP::fillTestDS(int sampleCnt)\n&#123;\n\twhile(sampleCnt--)\n\t&#123;\n\t\ttestDS.push_back(Data());\n\t&#125;\n&#125;\n\n/* 清空测试使用数据集 */\nvoid BP::clearTestDS()\n&#123;\n\tvector&lt;Data&gt; v;\n\ttestDS.swap(v);\n&#125;\n\n/* 预测 */\nvoid BP::forward(int index, bool test)\n&#123;\n\t/* 隐含层 */\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tu[i] = 0;\n\t\tfor(int j = 0 ; j &lt; I ; j++)\n\t\t&#123;\n\t\t\tdouble x = test ? testDS[index].x[j] : trainDS[index].x[j];\n\t\t\t/* 积累输入 */\n\t\t\tu[i] += w[j][i]*x;\n\t\t&#125;\n\t\tu[i] += th[i];\n\t\t/* Sigmoid函数作为激活函数 */\n\t\tu[i] = 1 / (1 + exp(-1*u[i]));\n\t&#125;\n\t/* 输出层 */\n\tfor(int i = 0 ; i &lt; O ; i++)\n\t&#123;\n\t\ty[i] = 0;\n\t\tfor(int j = 0 ; j &lt; H ; j++)\n\t\t&#123;\n\t\t\t/* 积累输入 */\n\t\t\ty[i] += v[j][i]*u[j];\n\t\t&#125;\n\t\ty[i] += to[i];\n\t\t/* 分类：Sigmoid函数作为激活函数 */\n\t\tif (!regression)\n\t\t&#123;\n\t\t\ty[i] = 1 / (1 + exp(-1*y[i]));\n\t\t&#125;\n\t&#125;\n&#125;\n\n/* 调整 */\nvoid BP::backward(int index)\n&#123;\n\t/* 计算隐含层与输出层间权重调整值 */\n\tfor(int i = 0 ; i &lt; O ; i++)\n\t&#123;\n\t\t/* 拟合：计算输出层学习误差 */\n\t\tif (regression)\n\t\t&#123;\n\t\t\tdv[i] = y[i] - trainDS[index].d[i];\n\t\t&#125;\n\t\t/* 分类：计算输出层学习误差 */\n\t\telse\n\t\t&#123;\n\t\t\tdv[i] = (y[i] - trainDS[index].d[i])*y[i]*(1 - y[i]);\n\t\t&#125;\n\t&#125;\n\t/* 计算输入层与隐含层间权重调整值 */\n\tdouble t;\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tt = 0;\n\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t&#123;\n\t\t\tt += dv[j]*v[i][j];\n\t\t&#125;\n\t\tdw[i] = t*u[i]*(1 - u[i]);\n\t&#125;\n\t/* 调整隐含层与输出层间权重 */\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t&#123;\n\t\t\tv[i][j] -= LR*dv[j]*u[i];\n\t\t&#125;\n\t&#125;\n\t/* 调整输出层偏置 */\n\tfor(int i = 0 ; i &lt; O ; i++)\n\t&#123;\n\t\tto[i] -= LR2*dv[i];\n\t&#125;\n\t/* 调整输入层与隐含层间权重 */\n\tfor(int i = 0 ; i &lt; I ; i++)\n\t&#123;\n\t\tfor(int j = 0 ; j &lt; H ; j++)\n\t\t&#123;\n\t\t\tw[i][j] -= LR*dw[j]*trainDS[index].x[i];\n\t\t&#125;\n\t&#125;\n\t/* 调整隐含层偏置 */\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tth[i] -= LR2*dw[i];\n\t&#125;\n&#125;\n\n/*\n * 类构造函数，初始化BP神经网络结构和训练参数\n * int _I：\t\t输入参数数目，包括偏置值对应的参数-1。\n * int _O：\t\t输出值数目。\n * int A：\t\t隐含层调整因子（1~10）。\n * double _LR：\t\t权重学习速率（0.01~0.8）。\n * double _LR2：\t偏置学习速率（0.01~0.8）。\n * double _LRDecay：\t学习速率衰减率（每次衰减与当前LR相乘）。\n * double _C：\t\t误差函数收敛阈值。\n * bool regression：\t拟合标识。\n */\nBP::BP(int _I, int _O, int A, double _LR, double _LR2, double _LRDecay, double _C, bool _regression)\n&#123;\n\t/* ========== 初始化常数 ========== */\n\tI = _I;\n\tH = ceil(sqrt(_I + _O)) + A;\n\tO = _O;\n\tLR = _LR;\n\tLR2 = _LR2;\n\tLRDecay = _LRDecay;\n\tC = _C;\n\tregression = _regression;\n\t/* ========== 初始化容器 ========== */\n\tsrand((unsigned)time(NULL));\n\t/* 初始化w */\n\tw = new double*[I];\n\tfor(int i = 0 ; i &lt; I ; i++)\n\t&#123;\n\t\tw[i] = new double[H];\n\t\tfor(int j = 0 ; j &lt; H ; j++)\n\t\t&#123;\n\t\t\tw[i][j] = rand()/(double)RAND_MAX;\n\t\t&#125;\n\t&#125;\n\t/* 初始化dw */\n\tdw = new double[H];\n\t/* 初始化th */\n\tth = new double[H];\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tth[i] = rand()/(double)RAND_MAX;\n\t&#125;\n\t/* 初始化u */\n\tu = new double[H];\n\t/* 初始化v */\n\tv = new double*[H];\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tv[i] = new double[O];\n\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t&#123;\n\t\t\tv[i][j] = rand()/(double)RAND_MAX;\n\t\t&#125;\n\t&#125;\n\t/* 初始化dv */\n\tdv = new double[O];\n\t/* 初始化to */\n\tto = new double[O];\n\tfor(int i = 0 ; i &lt; O ; i++)\n\t&#123;\n\t\tto[i] = rand()/(double)RAND_MAX;\n\t&#125;\n\t/* 初始化y */\n\ty = new double[O];\n&#125;\n\n/* 类析构函数，释放容器分配的堆空间 */\nBP::~BP()\n&#123;\n\t/* 释放w */\n\tfor(int i = 0 ; i &lt; I ; i++)\n\t&#123;\n\t\tdelete []w[i];\n\t&#125;\n\tdelete []w;\n\t/* 释放dw */\n\tdelete []dw;\n\t/* 释放th */\n\tdelete []th;\n\t/* 释放u */\n\tdelete []u;\n\t/* 释放v */\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tdelete []v[i];\n\t&#125;\n\tdelete []v;\n\t/* 释放dv */\n\tdelete []dv;\n\t/* 释放to */\n\tdelete []to;\n\t/* 释放y */\n\tdelete []y;\n&#125;\n\n/* 使用指定数目的样本训练指定数目次循环，返回最后的误差函数值 */\ndouble BP::train(int sampleCnt, int trainCnt)\n&#123;\n\tfillTrainDS(sampleCnt);\n\tdouble e;\n\twhile(trainCnt--)\n\t&#123;\n\t\te = 0;\n\t\tfor(int i = 0 ; i &lt; trainDS.size() ; i++)\n\t\t&#123;\n\t\t\t/* 预测 */\n\t\t\tforward(i);\n\t\t\t/* 误差积累 */\n\t\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t\t&#123;\n\t\t\t\te += pow(y[j] - trainDS[i].d[j], 2.0);\n\t\t\t&#125;\n\t\t\t/* 调整 */\n\t\t\tbackward(i);\n\t\t&#125;\n\t\te /= 2*sampleCnt;\n\t\t/* 学习速率衰减 */\n\t\tif (LR &gt; 0.01)\n\t\t&#123;\n\t\t\tLR *= LRDecay;\n\t\t&#125;\n\t&#125;\n\tclearTrainDS();\n\treturn e;\n&#125;\n\n/*\n * 使用指定数目的样本循环训练。\n * 误差函数值进入可接受范围判定收敛并停止训练；\n * 到达最大训练次数时停止训练。\n * 返回是否收敛。\n */\nbool BP::trainTillConvergent(int sampleCnt, int maxEpoch)\n&#123;\n\tfillTrainDS(sampleCnt);\n\tdouble e;\n\tfor(;;)\n\t&#123;\n\t\te = 0;\n\t\tfor(int i = 0 ; i &lt; trainDS.size() ; i++, maxEpoch--)\n\t\t&#123;\n\t\t\tif (!maxEpoch)\n\t\t\t&#123;\n\t\t\t\tclearTrainDS();\n\t\t\t\treturn false;\n\t\t\t&#125;\n\t\t\t/* 预测 */\n\t\t\tforward(i);\n\t\t\t/* 误差积累 */\n\t\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t\t&#123;\n\t\t\t\te += pow(y[j] - trainDS[i].d[j], 2.0);\n\t\t\t&#125;\n\t\t\t/* 调整 */\n\t\t\tbackward(i);\n\t\t&#125;\n\t\t/* 判定收敛，中止训练 */\n\t\tif (e/(2*sampleCnt) &lt; C)\n\t\t&#123;\n\t\t\tclearTrainDS();\n\t\t\treturn true;\n\t\t&#125;\n\t\t/* 学习速率衰减 */\n\t\tif (LR &gt; 0.01)\n\t\t&#123;\n\t\t\tLR *= LRDecay;\n\t\t&#125;\n\t&#125;\n\tclearTrainDS();\n\treturn false;\n&#125;\n\n/* 生成指定数目组数据测试当前神经网络 */\nvoid BP::testNetwork(int testCnt)\n&#123;\n\tfillTestDS(testCnt);\n\tdouble e = 0;\n\tfor(int i = 0 ; i &lt; testCnt ; i++)\n\t&#123;\n\t\tforward(i, true);\n\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t&#123;\n\t\t\te += pow(y[j] - testDS[i].d[j], 2.0);\n\t\t&#125;\n\t&#125;\n\tcout &lt;&lt; testCnt &lt;&lt; &quot;组数据测试预测值相对期望值方差为：&quot; &lt;&lt; e/(2*testCnt) &lt;&lt; endl;\n\tclearTestDS();\n&#125;\n\n/* 保存当前神经网络，即两个权重数组 */\nvoid BP::saveNetwork(string wPath, string vPath)\n&#123;\n\tofstream fout_w(wPath);\n\tif(fout_w == NULL)\n\t&#123;\n\t\tcout &lt;&lt; &quot;打开文件失败&quot; &lt;&lt; endl;\n\t\treturn;\n\t&#125;\n\tfor(int i = 0 ; i &lt; I ; i++)\n\t&#123;\n\t\tfor(int j = 0 ; j &lt; H ; j++)\n\t\t&#123;\n\t\t\tfout_w &lt;&lt; w[i][j] &lt;&lt; &#39;\\t&#39;;\n\t\t&#125;\n\t\tfout_w &lt;&lt; &#39;\\n&#39;;\n\t&#125;\n\tfout_w.close();\n\tofstream fout_v(vPath);\n\tif(fout_v == NULL)\n\t&#123;\n\t\tcout &lt;&lt; &quot;打开文件失败&quot; &lt;&lt; endl;\n\t\treturn;\n\t&#125;\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t&#123;\n\t\t\tfout_v &lt;&lt; v[i][j] &lt;&lt; &#39;\\t&#39;;\n\t\t&#125;\n\t\tfout_v &lt;&lt; &#39;\\n&#39;;\n\t&#125;\n\tfout_v.close();\n&#125;\n\n/* 载入两个权重数组，还原神经网络 */\nvoid BP::loadNetwork(string wPath, string vPath)\n&#123;\n\tifstream fin_w(wPath);\n\tif(fin_w == NULL)\n\t&#123;\n\t\tcout &lt;&lt; &quot;打开文件失败&quot; &lt;&lt; endl;\n\t\treturn;\n\t&#125;\n\tfor(int i = 0 ; i &lt; I ; i++)\n\t&#123;\n\t\tfor(int j = 0 ; j &lt; H ; j++)\n\t\t&#123;\n\t\t\tfin_w &gt;&gt; w[i][j];\n\t\t&#125;\n\t&#125;\n\tfin_w.close();\n\tifstream fin_v(vPath);\n\tif(fin_v == NULL)\n\t&#123;\n\t\tcout &lt;&lt; &quot;打开文件失败&quot; &lt;&lt; endl;\n\t\treturn;\n\t&#125;\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tfor(int j = 0 ; j &lt; O ; j++)\n\t\t&#123;\n\t\t\tfin_v &gt;&gt; v[i][j];\n\t\t&#125;\n\t&#125;\n\tfin_v.close();\n&#125;\n\n/* 使用指定数据预测输出值并返回输出值 */\nvector&lt;double&gt; BP::runNetwork(vector&lt;double&gt; x)\n&#123;\n\t/* 隐含层 */\n\tfor(int i = 0 ; i &lt; H ; i++)\n\t&#123;\n\t\tu[i] = 0;\n\t\tfor(int j = 0 ; j &lt; x.size() ; j++)\n\t\t&#123;\n\t\t\t/* 积累输入 */\n\t\t\tu[i] += w[j][i]*x[j];\n\t\t&#125;\n\t\tu[i] += th[i];\n\t\t/* Sigmoid函数作为激活函数 */\n\t\tu[i] = 1 / (1 + exp(-1*u[i]));\n\t&#125;\n\tvector&lt;double&gt; o;\n\tdouble y;\n\t/* 输出层 */\n\tfor(int i = 0 ; i &lt; O ; i++)\n\t&#123;\n\t\ty = 0;\n\t\tfor(int j = 0 ; j &lt; H ; j++)\n\t\t&#123;\n\t\t\t/* 积累输入 */\n\t\t\ty += v[j][i]*u[j];\n\t\t&#125;\n\t\ty += to[i];\n\t\t/* 分类：Sigmoid函数作为激活函数 */\n\t\tif (!regression)\n\t\t&#123;\n\t\t\ty = 1 / (1 + exp(-1*y));\n\t\t&#125;\n\t\to.push_back(y);\n\t&#125;\n\treturn o;\n&#125;\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 类的测试方法里无非是根据实际问题初始化类，做一系列的训练，调整参数，慢慢提高预测精度，test.cpp：</p>\n<pre><code class=\"hljs\">#include &quot;BP.h&quot;\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nint main()\n&#123;\n\t/*\n\t * 类构造函数，初始化BP神经网络结构和训练参数\n\t * int _I：\t\t输入参数数目，包括偏置值对应的参数-1。\n\t * int _O：\t\t输出值数目。\n\t * int A：\t\t隐含层调整因子（1~10）。\n\t * double _LR：\t\t学习速率（0.01~0.8）。\n\t * double _LR2：\t偏置学习速率（0.01~0.8）。\n\t * double _LRDecay：\t学习速率衰减率（每次衰减与当前LR相乘）。\n\t * double _C：\t\t误差函数收敛阈值。\n\t * bool regression：\t拟合标识。\n\t */\n\tBP o(7, 2, 8, 0.2, 0.02, 0.99, 3.2e-2, false);\n\n\t// o.loadNetwork();\n\n\t/* 使用指定数目的样本训练指定数目次循环，返回最后的误差函数值 */\n\tfor(int i = 0 ; i &lt; 20 ; i++)\n\t&#123;\n\t\tcout &lt;&lt; &quot;误差函数值：&quot; &lt;&lt; o.train(1000, 2e3) &lt;&lt; endl;\n\t&#125;\n\n\t/*\n\t * 使用指定数目的样本循环训练。\n\t * 误差函数值进入可接受范围判定收敛并停止训练；\n\t * 到达最大训练次数时停止训练。\n\t * 返回是否收敛。\n\t */\n\t// bool success = o.trainTillConvergent(1000, 1e5*1000);\n\t// cout &lt;&lt; &quot;收敛：&quot; &lt;&lt; (success ? &quot;success&quot; : &quot;fail&quot;) &lt;&lt; endl;\n\n\to.testNetwork(1000);\n\n\to.saveNetwork();\n\n\tvector&lt;double&gt; X;\n\tdouble x = 0.1;\n\tdouble y = 0.1;\n\tdouble z = 0.1;\n\tdouble a = 1;\n\tdouble b = 1;\n\tdouble c = 1;\n\tdouble d = -1;\n\tX.push_back((x + 10)/20.0);\n\tX.push_back((y + 10)/20.0);\n\tX.push_back((z + 10)/20.0);\n\tX.push_back((a + 10)/20.0);\n\tX.push_back((b + 10)/20.0);\n\tX.push_back((c + 10)/20.0);\n\tX.push_back((d + 10)/20.0);\n\tvector&lt;double&gt; Y = o.runNetwork(X);\n\t/* 拟合 */\n\t// cout &lt;&lt; &quot;期望值：&quot; &lt;&lt; z - (a*x + b*y + d)/(-c) &lt;&lt; endl;\n\t// cout &lt;&lt; &quot;输出值：&quot; &lt;&lt; Y[0] &lt;&lt; endl;\n\t/* 分类 */\n\tcout &lt;&lt; &quot;期望值：&quot; &lt;&lt; (z &gt; (a*x + b*y + d)/(-c) ? &quot;1\\t0&quot; : &quot;0\\t1&quot;) &lt;&lt; endl;\n\tcout &lt;&lt; &quot;输出值：&quot; &lt;&lt; Y[0] &lt;&lt; &#39;\\t&#39; &lt;&lt; Y[1] &lt;&lt; endl;\n\n\treturn 0;\n&#125;\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 点分类问题我先尝试了函数拟合方式，训练情况很糟糕，动辄上十万的方差。后来改用分类方式，降低训练要求，方差立马降下来了。下午例会就要做报告了，我没有训练到充分收敛，但是离平面比较远的点判断正确率已经很高了。下面提供平面分类点集问题当前训练进度下，输入层与隐含层间权重数组、隐含层与输出层间权重数组的保存文件。</p>\n<p>&nbsp;&nbsp;&nbsp; 输入层与隐含层间权重数组wNetwork：</p>\n<pre><code class=\"hljs\">0.15513\t0.56829\t0.461073\t0.27552\t0.44408\t0.144657\t0.95798\t0.335522\t0.325092\t0.254423\t0.79217\n-1.06417\t-0.0241895\t3.99915\t0.228275\t-0.483658\t6.68402\t-1.61574\t0.753689\t4.14272\t-4.58112\t-3.09481\n3.70442\t-3.7423\t0.0852013\t-0.564758\t-4.99619\t1.29436\t-4.19077\t-1.75579\t-0.568722\t-2.57748\t1.20149\n-0.311858\t2.12485\t0.494471\t-3.32854\t-0.393046\t1.12112\t-0.585242\t-3.45739\t-0.687711\t-1.37346\t-0.860532\n1.29214\t0.74987\t4.41025\t-1.37666\t0.268284\t6.72309\t2.0978\t-1.8259\t4.20105\t-6.15165\t5.04569\n-3.76331\t-3.22432\t-0.450262\t1.77063\t-5.89358\t3.40051\t3.64122\t1.30879\t-0.98291\t-0.931413\t-1.72719\n9.53636\t9.64562\t9.82743\t13.4779\t-12.3668\t12.5147\t8.37658\t6.32437\t-10.263\t-13.8476\t-5.31933\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 隐含层与输出层间权重数组vNetwork：</p>\n<pre><code class=\"hljs\">31.4711\t-31.5094\n27.536\t-27.5713\n-25.7692\t25.7999\n-27.4329\t27.467\n-17.6709\t17.6933\n-18.464\t17.5204\n25.8167\t-25.8471\n-23.2151\t23.2434\n27.3533\t-27.387\n26.9126\t-26.9519\n-24.7905\t24.8209\n</code></pre>\n<p>&nbsp;&nbsp;&nbsp; 这个类我还做过a+b问题的函数拟合，收敛非常顺利，精度很高。<br>&nbsp;&nbsp;&nbsp; <strong>后来，例会上提任务的博士说他明明是要我们寻找分割点集的平面，我把问题泛化成寻找点与平面位置关系规则的问题了，能用泛化能力不太强的BP整的差不多收敛也是不容易。我表示生无可恋……</strong></p>\n<p>&nbsp;&nbsp;&nbsp; <strong>读者注意一下，上一篇BP网络的理论介绍中，隐层与输入层之间的权重调整值的推导有一些问题，少乘了一个wij。推导已经改过来了，但是比较忙没空改代码，有需要的同学请自己参照上一篇博文修改一下代码。OTZ</strong></p>\n",
            "tags": [
                "C++",
                "BP神经网络",
                "机器学习"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2015/10/09/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/",
            "url": "https://blog.bipedalbit.net/2015/10/09/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/",
            "title": "BP神经网络",
            "date_published": "2015-10-09T04:45:11.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 组里要求新人手写个BP神经网络练练手先，具体要求是用一个平面把空间点集二分类，建模时我发现这个问题比起分类来更接近一个多元函数拟合，当然分类也不是不行，机器学习的要求更低，更容易有更好的结果。这都是后话，这篇文我决定先来总结一下BP神经网络的概念，也算是前几天的学习笔记了。</p>\n<span id=\"more\"></span>\n<h1 id=\"1-机器学习\"><a href=\"#1-机器学习\" class=\"headerlink\" title=\"1. 机器学习\"></a>1. 机器学习</h1><p>&nbsp;&nbsp;&nbsp; 百度百科说机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。简而言之，机器学习专门研究计算机怎样<strong>模拟或实现人类的学习行为</strong>，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。机器学习是人工智能的核心，是使计算机具有智能的<strong>根本途径</strong>，其应用遍及人工智能的各个领域，它主要使用<strong>归纳、综合而不是演绎</strong>。<br>&nbsp;&nbsp;&nbsp; 机器学习的方向很多，其历史和兴衰说来话长，这里引入机器学习的概念只是为了做个铺垫，就不展开了。</p>\n<h1 id=\"2-人工神经网络\"><a href=\"#2-人工神经网络\" class=\"headerlink\" title=\"2. 人工神经网络\"></a>2. 人工神经网络</h1><p>&nbsp;&nbsp;&nbsp; 人工神经网络（Artificial Neural Network，ANN ）是人工智能的一支，是机器学习的一个方向，试图通过数学模型和适当的数据结构还原智能的温床——神经系统。我们知道人类大脑信息的传递、对外界刺激产生反应都是由一种特化的细胞——神经元来负责，人脑就是由上百亿个神经元构成。这些神经元之间并不孤立而且联系很密切，每个神经元平均与几千个神经元相连接，因此构成了一个神经网络。<br>&nbsp;&nbsp;&nbsp; 刺激在神经网络中的传播是遵循一定的规则的，一个神经元并非每次接到其他神经传递过来的刺激都产生反应。它首先会将与其相邻的神经元传来的刺激进行积累，到一定的时候产生自己的刺激将其传递给一些与它相邻的神经元。遵循同样规则工作着的百亿个的神经元完成了人脑对外界的反应。而人脑对外界刺激的学习过程则是通过这些神经元之间联系的建立及其强度的调整来完成的。<br>&nbsp;&nbsp;&nbsp; 当然，事实上，上述生物模型是对真正神经网络工作机制的一种简化，这种简化的生物模型可以推广至机器学习中，并以数学模型为理论基础，用不同的编程方式来实现。这种对神经网络的模拟，称为人工神经网络。</p>\n<h2 id=\"2-1-人工神经元（Artificial-Neuron，AN）模型\"><a href=\"#2-1-人工神经元（Artificial-Neuron，AN）模型\" class=\"headerlink\" title=\"2.1. 人工神经元（Artificial Neuron，AN）模型\"></a>2.1. 人工神经元（Artificial Neuron，AN）模型</h2><p>&nbsp;&nbsp;&nbsp; AN是ANN的基本元素之一，其原理如下图。<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.jpg\" alt=\"图1 人工神经元模型\" title=\"人工神经元模型\"><br>&nbsp;&nbsp;&nbsp; 图中x1~xn是从其他神经元传来的输入信号，wij表示表示从神经元j到神经元i的连接权值，θ表示一个偏置( bias )。则神经元i的输出与输入的关系表示为：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E5%85%83%E8%BE%93%E5%85%A5%E7%A7%AF%E7%B4%AF1.png\" title=\"净激活\"><br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E5%85%83%E8%BE%93%E5%87%BA3.png\" title=\"激活输出\"><br>&nbsp;&nbsp;&nbsp; 图中 yi表示神经元i的输出，函数f称为激活函数 ( Activation Function )，net称为净激活(net activation)。<br>&nbsp;&nbsp;&nbsp; 如果令一个输入参数x0为-1，对应权重为偏置θ，则可以把偏置项合并进净激活net：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E5%85%83%E8%BE%93%E5%85%A5%E7%A7%AF%E7%B4%AF2.png\" title=\"合并偏置的净激活\"><br>&nbsp;&nbsp;&nbsp; 若令X为输入向量，W为权重向量，则有：</p>\n<p align=\"center\"><span style=\"font-size:16px\">X = [ x0 , x1 , x2 , ....... , xn ]</span></p>\n![](https://cdn.minio.bipedalbit.net/bipedalbit-net-images/人工神经网络/权重向量.png \"权重向量\")\n&nbsp;&nbsp;&nbsp; 将神经元的输出表示为向量相乘的形式则有：\n![](https://cdn.minio.bipedalbit.net/bipedalbit-net-images/人工神经网络/神经元输出1.png \"净激活\")\n![](https://cdn.minio.bipedalbit.net/bipedalbit-net-images/人工神经网络/神经元输出2.png \"神经元输出\")\n&nbsp;&nbsp;&nbsp; 若神经元的净激活net为正，称该神经元处于激活状态或兴奋状态，若净激活net为负，则称神经元处于抑制状态。\n&nbsp;&nbsp;&nbsp; 图1中的这种“阈值加权和”的神经元模型称为M-P模型 ( McCulloch-Pitts Model )，也称为神经网络的一个处理单元( PE, Processing Element )。\n## 2.2. 常用激活函数\n\n<p>&nbsp;&nbsp;&nbsp; 前面的神经元模型讲解中也提到了激活函数，事实上，激活函数是人工神经网络中的一个重要环节。下面介绍一些常用的激活函数。</p>\n<h3 id=\"2-2-1-线性函数（Liner-Function）\"><a href=\"#2-2-1-线性函数（Liner-Function）\" class=\"headerlink\" title=\"2.2.1. 线性函数（Liner Function）\"></a>2.2.1. 线性函数（Liner Function）</h3><p><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0.png\" title=\"线性函数\"></p>\n<h3 id=\"2-2-2-斜面函数（Ramp-Function）\"><a href=\"#2-2-2-斜面函数（Ramp-Function）\" class=\"headerlink\" title=\"2.2.2. 斜面函数（Ramp Function）\"></a>2.2.2. 斜面函数（Ramp Function）</h3><p><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%96%9C%E9%9D%A2%E5%87%BD%E6%95%B0.png\" title=\"斜面函数\"></p>\n<h3 id=\"2-2-3-阈值函数（Threshold-Function）\"><a href=\"#2-2-3-阈值函数（Threshold-Function）\" class=\"headerlink\" title=\"2.2.3. 阈值函数（Threshold Function）\"></a>2.2.3. 阈值函数（Threshold Function）</h3><p><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E9%98%88%E5%80%BC%E5%87%BD%E6%95%B0.png\" title=\"阈值函数\"></p>\n<h3 id=\"2-2-4-S形函数（Sigmoid-Function）\"><a href=\"#2-2-4-S形函数（Sigmoid-Function）\" class=\"headerlink\" title=\"2.2.4. S形函数（Sigmoid Function）\"></a>2.2.4. S形函数（Sigmoid Function）</h3><p><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Sigmoid-S%E5%BD%A2%E5%87%BD%E6%95%B0.png\" title=\"S形函数\"><br>&nbsp;&nbsp;&nbsp; 导函数：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/S%E5%BD%A2%E5%87%BD%E6%95%B0%E5%AF%BC%E6%95%B0.png\" title=\"S形函数导函数\"></p>\n<h3 id=\"2-2-5-双极S形函数\"><a href=\"#2-2-5-双极S形函数\" class=\"headerlink\" title=\"2.2.5. 双极S形函数\"></a>2.2.5. 双极S形函数</h3><p><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8C%E6%9E%81S%E5%BD%A2%E5%87%BD%E6%95%B0.png\" title=\"双极S形函数\"><br>&nbsp;&nbsp;&nbsp; 导函数：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8C%E6%9E%81S%E5%BD%A2%E5%87%BD%E6%95%B0%E5%AF%BC%E6%95%B0.png\" title=\"双极S形函数导函数\"><br>&nbsp;&nbsp;&nbsp; S形函数与双极S形函数图像：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/S%E5%BD%A2%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8F%8C%E6%9E%81S%E5%BD%A2%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png\" alt=\"图2 S形函数与双极S形函数图像\" title=\"S形函数与双极S形函数图像\"><br>&nbsp;&nbsp;&nbsp; 双极S形函数与S形函数主要区别在于函数的值域，双极S形函数值域是(-1,1)，而S形函数值域是(0,1)。<br>&nbsp;&nbsp;&nbsp; 由于S形函数与双极S形函数都是可导的，因此适合用在BP神经网络中。（BP算法要求激活函数可导）</p>\n<h2 id=\"2-3-神经网络模型\"><a href=\"#2-3-神经网络模型\" class=\"headerlink\" title=\"2.3. 神经网络模型\"></a>2.3. 神经网络模型</h2><p>&nbsp;&nbsp;&nbsp; 根据神经元互联方式的不同，神经网络可以分为3种常见类型。</p>\n<h3 id=\"2-3-1-前馈神经网络（Feedforward-Neural-Networks）\"><a href=\"#2-3-1-前馈神经网络（Feedforward-Neural-Networks）\" class=\"headerlink\" title=\"2.3.1. 前馈神经网络（Feedforward Neural Networks）\"></a>2.3.1. 前馈神经网络（Feedforward Neural Networks）</h3><p>&nbsp;&nbsp;&nbsp; 前馈网络也称前向网络。这种网络只在训练过程会有反馈信号，而在分类过程中数据只能向前传送，直到到达输出层，层间没有向后的反馈信号，因此被称为前馈网络。感知机( perceptron)与BP神经网络属于前馈网络。<br>&nbsp;&nbsp;&nbsp; 下图是一个3层的前馈神经网络，其中第一层是输入单元，第二层称为隐含层，第三层称为输出层（输入单元不是神经元，因此图中有2层神经元）。<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg\" alt=\"图3 前馈神经网络\" title=\"前馈神经网络\"><br>&nbsp;&nbsp;&nbsp; N层的前馈神经网络每层的输入参数都要按权重积累，得到净激活后由激活函数激活输出。<br>&nbsp;&nbsp;&nbsp; 比如，一个三层的前馈神经网络的输出为<code>F3( F2 ( F1( XW1 ) W2 ) W3 )</code>。当F1、F2、F3都选用线性激活函数，输出将是线性函数。<br>&nbsp;&nbsp;&nbsp; <strong>所以，如果要做高次函数的拟合，应该选用适当的非线性激活函数。</strong></p>\n<h3 id=\"2-3-2-反馈神经网络（Feedback-Neural-Networks）\"><a href=\"#2-3-2-反馈神经网络（Feedback-Neural-Networks）\" class=\"headerlink\" title=\"2.3.2. 反馈神经网络（Feedback Neural Networks）\"></a>2.3.2. 反馈神经网络（Feedback Neural Networks）</h3><p>&nbsp;&nbsp;&nbsp; 反馈型神经网络是一种从输出到输入具有反馈连接的神经网络，其结构比前馈网络要复杂得多。典型的反馈型神经网络有Elman网络和Hopfield网络。<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg\" alt=\"图4 反馈神经网络\" title=\"反馈神经网络\"></p>\n<h3 id=\"2-3-3-竞争学习网络（Competitive-Learning-Network）\"><a href=\"#2-3-3-竞争学习网络（Competitive-Learning-Network）\" class=\"headerlink\" title=\"2.3.3. 竞争学习网络（Competitive Learning Network）\"></a>2.3.3. 竞争学习网络（Competitive Learning Network）</h3><p>&nbsp;&nbsp;&nbsp; 竞争学习网络是一种无监督学习网络。它通过自动寻找样本中的内在规律和本质属性，自组织、自适应地改变网络参数与结构。典型的竞争学习网络有SOM。<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%87%AA%E7%BB%84%E7%BB%87%E7%BD%91%E7%BB%9C.png\" alt=\"图5 竞争学习网络\" title=\"竞争学习网络\"></p>\n<h3 id=\"2-3-4-神经网络的状态\"><a href=\"#2-3-4-神经网络的状态\" class=\"headerlink\" title=\"2.3.4. 神经网络的状态\"></a>2.3.4. 神经网络的状态</h3><p>&nbsp;&nbsp;&nbsp; 神经网络的状态分学习（训练）状态和工作状态两种。<br>&nbsp;&nbsp;&nbsp; 学习状态又分为两种方式：监督学习（Supervised Learning）和非监督（Unsupervised Learning）学习。<br>&nbsp;&nbsp;&nbsp; 监督学习算法将一组训练数据集送入网络，根据网络的实际输出与期望输出间的差别来调整连接权。达到预设训练次数或者样本整体误差不超过预设范围（收敛）时学习结束。BP算法就是一种出色的监督学习算法。<br>&nbsp;&nbsp;&nbsp; 非监督学习算法抽取样本集合中蕴含的统计特性，并以神经元之间的联接权的形式存于网络中。Hebb学习律是一种经典的非监督学习算法。</p>\n<h3 id=\"2-3-5-非监督学习算法：Hebb学习律\"><a href=\"#2-3-5-非监督学习算法：Hebb学习律\" class=\"headerlink\" title=\"2.3.5. 非监督学习算法：Hebb学习律\"></a>2.3.5. 非监督学习算法：Hebb学习律</h3><p>&nbsp;&nbsp;&nbsp; Hebb算法核心思想是，当两个神经元同时处于激发状态时两者间的连接权会被加强，否则被减弱。<br>&nbsp;&nbsp;&nbsp; Hebb学习律可以表示为：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Hebb%E5%AD%A6%E4%B9%A0%E5%BE%8B.png\" title=\"Hebb学习律\"><br>&nbsp;&nbsp;&nbsp; 其中wij表示神经元j到神经元i的连接权，yi与yj为两个神经元的输出，a是表示学习速率常数。若yi与yj同时被激活，即yi与yj同号，那 么wij将增大；若yi被激活，而yj处于抑制状态，即yi为正yj为负，那么wij将变小。</p>\n<h3 id=\"2-3-6-监督学习算法：Delta学习规则\"><a href=\"#2-3-6-监督学习算法：Delta学习规则\" class=\"headerlink\" title=\"2.3.6. 监督学习算法：Delta学习规则\"></a>2.3.6. 监督学习算法：Delta学习规则</h3><p>&nbsp;&nbsp;&nbsp; Delta学习规则的数学表达：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Delta%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99.png\" title=\"Delta学习规则\"><br>&nbsp;&nbsp;&nbsp; 其中wij表示神经元j到神经元i的连接权，di是神经元i的期望输出，yi是神经元i的实际输出，xj表示神经元j状态，若神经元j处于激活态则xj为 1，若处于抑制状态则xj为0或－1（依激活函数而定）。a是表示学习速率常数。假设xi为1，若di比yi大，那么Wij将增大，若di比yi小， 那么wij将变小。<br>&nbsp;&nbsp;&nbsp; Delta规则简单讲来就是：若神经元实际输出比期望输出大，则减小所有输入为正的连接的权重，增大所有输入为负的连接的权重。反之，若神经元实际输出比期望输出小，则增大所有输入为正的连接的权重，减小所有输入为负的连接的权重。这个增大或减小的幅度就根据上面的式子来计算。</p>\n<h1 id=\"3-BP神经网络\"><a href=\"#3-BP神经网络\" class=\"headerlink\" title=\"3. BP神经网络\"></a>3. BP神经网络</h1><h2 id=\"3-1-认识BP神经网络\"><a href=\"#3-1-认识BP神经网络\" class=\"headerlink\" title=\"3.1. 认识BP神经网络\"></a>3.1. 认识BP神经网络</h2><p>&nbsp;&nbsp;&nbsp; BP神经网络更清晰的称呼应该是使用BP算法学习的前馈神经网络。BP神经网络中的BP为Back Propagation（反向传播）的简写，最早它是由Rumelhart、McCelland等科学家于1986年提出来的，Rumelhart还在《Nature》上发表了一篇非常著名的文章《Learning representations by back-propagating errors》。随着时代的迁移，BP神经网络理论不断的得到改进、更新，现在无疑已成为了应用最为广泛的神经网络模型之一。BP网络具有很强的非线性映射能力，一个3层BP神经网络能够实现对任意非线性函数进行逼近（根据Kolrnogorov定理：任意[0,1]上的连续函数能被一个3层的神经网络实现，其中输入单元数为n，隐含层单元数为2n+1）。</p>\n<h2 id=\"3-2-隐含层的选取\"><a href=\"#3-2-隐含层的选取\" class=\"headerlink\" title=\"3.2. 隐含层的选取\"></a>3.2. 隐含层的选取</h2><p>&nbsp;&nbsp;&nbsp; 在BP神经网络中，输入层和输出层的节点个数都是确定的，而隐含层节点个数不确定。实际上，隐含层节点个数的多少对神经网络的性能是有影响的，有一个经验公式可以确定隐含层神经元数目：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP%E9%9A%90%E5%90%AB%E5%B1%82%E7%BB%8F%E9%AA%8C%E5%85%AC%E5%BC%8F.png\" title=\"BP隐含层经验公式\"><br>&nbsp;&nbsp;&nbsp; 其中，h为隐含层神经元数，m为输入结点数，n为输出层神经元数，a为1~10之间的调节常数。</p>\n<h2 id=\"3-3-正向预测\"><a href=\"#3-3-正向预测\" class=\"headerlink\" title=\"3.3. 正向预测\"></a>3.3. 正向预测</h2><p>&nbsp;&nbsp;&nbsp; 就是使输入数据样本通过一个两层前馈网络，输出即：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP%E5%87%80%E6%BF%80%E6%B4%BB.png\" title=\"BP净激活\"><br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP%E7%A5%9E%E7%BB%8F%E5%85%83%E8%BE%93%E5%87%BA.png\" title=\"BP神经元输出\"></p>\n<h2 id=\"3-4-反向误差传播\"><a href=\"#3-4-反向误差传播\" class=\"headerlink\" title=\"3.4. 反向误差传播\"></a>3.4. 反向误差传播</h2><p>&nbsp;&nbsp;&nbsp; BP算法是基于Widrow-Hoff学习规则的，可以看做delta学习规则的一种特殊情况。Widrow-Hoff学习规则是通过沿着相对误差平方和的最速下降方向（梯度方向），连续调整网络的权重和偏置。根据梯度下降法，权重矢量的修正正比于当前位置上E(w,b)的梯度的模，即E(w,b)在当前位置的偏导数。我们知道函数<code>z = f(x,y)</code>中，z对x或y的偏导数即z随x或y变化的程度或快慢，这正切合BP神经网络中每层输入参数权重的意义，也印证了梯度下降法的正确性。对于第j个输出神经元有：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/BP%E8%BE%93%E5%87%BA%E5%B1%82%E6%9D%83%E5%80%BC%E4%BF%AE%E6%AD%A3.png\" title=\"输出层权值修正\"><br>&nbsp;&nbsp;&nbsp; 其中i是隐含层神经元的序号。<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC1.png\" title=\"误差函数对权重的偏导\"><br>&nbsp;&nbsp;&nbsp; 其中n是输出层神经元数，m是隐含层神经元数，于是得到：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC2.png\" title=\"误差函数对权重偏导2\"><br>&nbsp;&nbsp;&nbsp; 已知Sigmoid函数的导函数：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/S%E5%BD%A2%E5%87%BD%E6%95%B0%E5%AF%BC%E6%95%B0.png\" title=\"Sigmoid函数导函数\"><br>&nbsp;&nbsp;&nbsp; 于是有：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC3.png\" title=\"误差函数对权重偏导3\"><br>&nbsp;&nbsp;&nbsp; 同理，对输出层偏置bj有：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC4.png\" title=\"输出层偏置\"><br>&nbsp;&nbsp;&nbsp; 以上就是著名的<strong>delta学习规则</strong>，通过改变神经元之间的连接权值来减少系统实际输出和期望输出的误差，这个规则又叫做<strong>Widrow-Hoff学习规则</strong>或者<strong>纠错学习规则</strong>。<br>&nbsp;&nbsp;&nbsp; 设k为输入层的节点序号，下面继续推导输入层与隐含层的权重修正：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC5.png\" title=\"误差函数对权重偏导4\"><br>&nbsp;&nbsp;&nbsp; 其中l是输入层节点数，yi也是前面推导隐含层与输出层间权重修正时的xi，于是有：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC6.png\" title=\"误差函数对权重偏导5\"><br>&nbsp;&nbsp;&nbsp; 同理，对隐含层偏置bi有：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC7.png\" title=\"隐含层偏置\"><br>&nbsp;&nbsp;&nbsp; 最后，输入层与隐含层间权重和偏置，隐含层与输出层间权重和偏置的修正值如下：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8E%A8%E5%AF%BC8.png\" title=\"最终修正值\"><br>&nbsp;&nbsp;&nbsp; 其中，eita1和eita2分别是权重学习速率和偏置学习速率。</p>\n<h2 id=\"3-5-注意点\"><a href=\"#3-5-注意点\" class=\"headerlink\" title=\"3.5. 注意点\"></a>3.5. 注意点</h2><ul>\n<li>输入数据要先进行归一化处理。例如使用Sigmoid函数作为激活函数时，输入参数如果分布在Sigmoid函数曲线的稳定区域中，即定义域的[-1,1]区间之外的区域，那么激活值将集中在0和1上，权重对输入参数的加权作用将失去意义，输入参数对权重的调整也将失去意义。</li>\n<li>学习速率大的时候收敛速度快精度小，收敛曲线颠簸；学习速率小收敛速度慢精度大，收敛曲线平滑。</li>\n<li>BP神经网络通常用于分类器和函数拟合，如果作为分类器，激活函数一般选用Sigmoid函数；如果做函数拟合，<strong>输出层</strong>的激活函数应该用线性函数即<code>f(x) = x</code>。</li>\n<li>BP神经网络训练过程中可以采用<strong>增量学习</strong>或<strong>批量学习</strong>，即分批临时生成新的训练数据样本并输入当前网络，分阶段逐步学习；或者预生成所有训练数据样本，学习到出结果为止。</li>\n</ul>\n<h2 id=\"3-6-缺陷\"><a href=\"#3-6-缺陷\" class=\"headerlink\" title=\"3.6. 缺陷\"></a>3.6. 缺陷</h2><ul>\n<li>BP神经网络训练时容易陷入局部极小值”假收敛“而得不到全局最优值，如果出现”假收敛“，只能重新随机初始化权重。</li>\n<li>BP神经网络需要的训练次数多，收敛比较慢。</li>\n<li>隐含层神经元数的选取缺乏理论指导。</li>\n<li>训练时学习新样本有遗忘旧样本的趋势，即“BP已死”这一说法的根源——泛化能力弱。</li>\n</ul>\n<h2 id=\"3-7-改进\"><a href=\"#3-7-改进\" class=\"headerlink\" title=\"3.7. 改进\"></a>3.7. 改进</h2><ul>\n<li>加入动量项，即记录上一次权重变化，通过变量因子确定继承上一次权重变化的程度，目的是使权重的调整方向不至于来回颠簸太厉害，表现在误差函数值上则是能令收敛曲线更趋平缓。</li>\n<li>自适应的调整学习因子，即当权重调整到一定程度，不降低学习程度就无法更精细的使神经网络中的关系靠近理想关系。这时需要适量的减小学习因子，一般采取给学习因子添加衰减率的方式。</li>\n<li>通过蚁群算法、遗传算法等算法优化BP神经网络。</li>\n</ul>\n",
            "tags": [
                "BP神经网络",
                "机器学习",
                "人工神经网络"
            ]
        }
    ]
}