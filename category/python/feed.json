{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hacking to the gate! • All posts by \"python\" category",
    "description": "Bipedal Bit's blog",
    "home_page_url": "https://blog.bipedalbit.net",
    "items": [
        {
            "id": "https://blog.bipedalbit.net/2016/05/08/FFmpeg%E6%89%B9%E9%87%8F%E6%8A%93%E5%B8%A7%E8%84%9A%E6%9C%AC/",
            "url": "https://blog.bipedalbit.net/2016/05/08/FFmpeg%E6%89%B9%E9%87%8F%E6%8A%93%E5%B8%A7%E8%84%9A%E6%9C%AC/",
            "title": "FFmpeg批量抓帧脚本",
            "date_published": "2016-05-08T08:29:47.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 上周一个学姐问我有没有兴趣毕业去她那做图像处理，抛给我个模式识别问题和一段4000+秒的mp4视频。大周末的我正犯五月病，就跟群里大佬问了下视频抓帧用什么合适，知道神奇的FFmpeg后顺手写了个python脚本做一下批量抓帧。至于为什么要用python不直接写shell，因为FFmpeg自带的批量抓帧命令是针对连续时间序列进行的，执行起来特别慢，用python是要做一下时间序列离散化，然后并行处理。</p>\n<span id=\"more\"></span>\n<h3 id=\"FFmpeg\"><a href=\"#FFmpeg\" class=\"headerlink\" title=\"FFmpeg\"></a>FFmpeg</h3><p>&nbsp;&nbsp;&nbsp; 先引用百度百科简单介绍下FFmpeg：</p>\n<blockquote>\n<p>FFmpeg是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。采用LGPL或GPL许可证。它提供了录制、转换以及流化音视频的完整解决方案。它包含了非常先进的音频&#x2F;视频编解码库libavcodec，为了保证高可移植性和编解码质量，libavcodec里很多codec都是从头开发的。</p>\n</blockquote>\n<p>&nbsp;&nbsp;&nbsp; 关于FFmpeg的业界地位，有很多视音频播放器是通过给FFmpeg加壳完成的。它是跨平台的，linux、win、mac os下都有发行版。想要安装，可以去<a href=\"http://ffmpeg.org/\">官网</a>看看。关于文档，我找到的总结、教程、手册都比较零散，官方的英语文档又让新手无从看起，这次我只查到够用的资料就放着了，如果读者找到比较全面的实用手册，欢迎留言。</p>\n<h3 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h3><p>&nbsp;&nbsp;&nbsp; 我试着直接执行FFmpeg的批量抓帧命令时发现特别慢，几乎总要花费目标视频一半的播放时间。但是单张抓帧，不论时间点在哪里，其实都很快。于是我的思路是把视频的总时长拿出来，然后获得一个均匀分布的时间点集合，最后统统扔给gevent的pool并行抓帧。gevent是python一个著名的coroutine（协程）框架，初衷是处理高并发的网络IO，要安装pip一下就好。思路很简单，脚本也很短，life is short, I choose python!</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span><br><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span><br><br><span class=\"hljs-keyword\">import</span> os<br><span class=\"hljs-keyword\">from</span> commands <span class=\"hljs-keyword\">import</span> getoutput<br><span class=\"hljs-keyword\">import</span> re<br><span class=\"hljs-keyword\">from</span> gevent.pool <span class=\"hljs-keyword\">import</span> Pool <span class=\"hljs-keyword\">as</span>  Gpool<br><span class=\"hljs-keyword\">from</span> time <span class=\"hljs-keyword\">import</span> time<br><br><span class=\"hljs-comment\"># get arguments</span><br>file_path = <span class=\"hljs-string\">&#x27;&#x27;</span><br>ouput_path = <span class=\"hljs-string\">&#x27;&#x27;</span><br>interval = <span class=\"hljs-number\">1</span><br><span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:<br>    file_path = raw_input(<span class=\"hljs-string\">&#x27;Vedio path: &#x27;</span>)<br>    <span class=\"hljs-keyword\">if</span> os.path.isfile(file_path):<br>        <span class=\"hljs-keyword\">break</span><br>    <span class=\"hljs-keyword\">else</span>:<br>        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;Not a file.&#x27;</span><br><span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:<br>    ouput_path = raw_input(<span class=\"hljs-string\">&#x27;Output path( current directory for default ) : &#x27;</span>)<br>    <span class=\"hljs-keyword\">if</span> ouput_path == <span class=\"hljs-string\">&#x27;&#x27;</span>:<br>        <span class=\"hljs-keyword\">break</span><br>    <span class=\"hljs-keyword\">if</span> os.path.exists(ouput_path) <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> os.path.isfile(ouput_path):<br>        ouput_path += <span class=\"hljs-string\">&#x27;/&#x27;</span><br>        <span class=\"hljs-keyword\">break</span><br>    <span class=\"hljs-keyword\">else</span>:<br>        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;Not a directory.&#x27;</span><br><span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:<br>    s = raw_input(<span class=\"hljs-string\">&#x27;Snap interval( 1 second for default ) : &#x27;</span>)<br>    <span class=\"hljs-keyword\">if</span> s == <span class=\"hljs-string\">&#x27;&#x27;</span>:<br>        interval = <span class=\"hljs-number\">1</span><br>        <span class=\"hljs-keyword\">break</span><br>    <span class=\"hljs-keyword\">if</span> re.<span class=\"hljs-keyword\">match</span>(<span class=\"hljs-string\">r&#x27;^[0-9]+(.[0-9]+)&#123;0,1&#125;$&#x27;</span>, s) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:<br>        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;Not a number.&#x27;</span><br>    <span class=\"hljs-keyword\">else</span>:<br>        interval = <span class=\"hljs-built_in\">float</span>(s)<br>        <span class=\"hljs-keyword\">break</span><br><br><span class=\"hljs-comment\"># get vedio duration via os.popen with &quot;ffmpeg -i&quot;</span><br>info = getoutput(<span class=\"hljs-string\">&#x27;ffmpeg -i &#x27;</span> + file_path)<br>dur = re.search(<span class=\"hljs-string\">r&#x27;(?&lt;=Duration: ).*?(?=,)&#x27;</span>, info).group(<span class=\"hljs-number\">0</span>).split(<span class=\"hljs-string\">&#x27;:&#x27;</span>)<br>dur = <span class=\"hljs-built_in\">int</span>(dur[<span class=\"hljs-number\">0</span>])*<span class=\"hljs-number\">3600</span> + <span class=\"hljs-built_in\">int</span>(dur[<span class=\"hljs-number\">1</span>])*<span class=\"hljs-number\">60</span> + <span class=\"hljs-built_in\">float</span>(dur[<span class=\"hljs-number\">2</span>])<br><span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;Vedio duration: %.2f seconds.&#x27;</span> % dur<br><br><span class=\"hljs-comment\"># make time stamps pool</span><br>time_stamp_pool = []<br>time_stamp = <span class=\"hljs-number\">0</span><br><span class=\"hljs-keyword\">while</span> time_stamp &lt; dur:<br>    time_stamp_pool.append(time_stamp)<br>    time_stamp += interval<br><br><span class=\"hljs-comment\"># os.system + gevent snap by batch</span><br>gpool = Gpool()<br>snap_cmd = <span class=\"hljs-string\">&#x27;ffmpeg -ss %f -i %s -nostats -loglevel 0 -q:v 2 -f image2 %s%d.jpg&#x27;</span> <span class=\"hljs-comment\"># execute quietly</span><br>n_snap = <span class=\"hljs-built_in\">len</span>(time_stamp_pool)<br><span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;%d frames to be snapped.&#x27;</span> % n_snap<br><span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;Handling...&#x27;</span><br>time0 = time()<br><span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> xrange(n_snap):<br>    gpool.spawn(os.system, snap_cmd % (time_stamp_pool[i], file_path, ouput_path, i))<br>gpool.join()<br>time_cost = time() - time0<br><span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;Done in %.2f seconds.&#x27;</span> % time_cost<br></code></pre></td></tr></table></figure>",
            "tags": [
                "python",
                "ffmpeg"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2016/01/06/%E5%BE%AE%E5%8D%9A%E7%88%AC%E8%99%AB%E4%B9%8B%E4%BB%A3%E7%90%86%E6%B1%A0/",
            "url": "https://blog.bipedalbit.net/2016/01/06/%E5%BE%AE%E5%8D%9A%E7%88%AC%E8%99%AB%E4%B9%8B%E4%BB%A3%E7%90%86%E6%B1%A0/",
            "title": "微博爬虫之代理池",
            "date_published": "2016-01-05T17:02:08.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 前一个版本的微博爬虫每发送一个HTTP请求就需要等待若干秒，模拟人类操作，避免引起服务器的注意，以至于每个请求平均耗时高达3秒。为了防止被服务器封禁，显然应该使用代理，伪装自己HTTP请求的来源。至于如何获取代理，如何使用这些代理，我进行了一些思考与探索，并完善上个版本的微博爬虫工具包，完成了新版的微博爬虫。</p>\n<span id=\"more\"></span>\n<p>&nbsp;&nbsp;&nbsp; 我的第一个思路是，找到提供大量免费代理的网站（自己找，本文不点明），爬取代理，然后用多线程的方式成组发送请求。虽然获取代理的过程很顺利，但是我很快发现了这个思路的一些问题。<br>&nbsp;&nbsp;&nbsp; 首先，成组多线程并发请求的话，<strong>下一页链接</strong>没法使用同一个序列，即可能需要先获取页数然后做页码区间划分，再把每个区间分配给不同的代理。可是如果某个代理中途失效了怎么办？就算不考虑这种情况，理论上，每组请求之间还是需要加平均3秒的延迟，实际上，受IO上限的影响，16线程的成组请求，等到全部响应完大概需要10+秒，这样一来组间延迟省掉也无所谓，可是即使在只使用优质代理的情况下每次请求的均摊时间开销也要1.+秒，虽然效率比不使用代理有所提高，但是我仍然不满意。<br>&nbsp;&nbsp;&nbsp; 随后我放弃了对多线程并发的执念，提出并完善了一个新思路——单线程爬虫＋带时间戳的代理池＋代理竞争。<br>&nbsp;&nbsp;&nbsp; 先解释单线程爬虫，有了代理池其实已经基本不用担心代理失效的问题，但是为了实现方便，我还是暂且选择了单线程爬虫。下一个版本也许会在整个爬虫外面套一层并发，也许会考虑使用协程（coroutine）技术。<br>&nbsp;&nbsp;&nbsp; 接下来是带时间戳的代理池和代理竞争。我们知道，每一个代理在发出一个请求后需要等待一个平均3秒的随机时间后才可以再次发送请求。那么，如果我们使用过一个代理之后就暂时给它上锁（实际上是打上一个时间戳），到时间后（当前时间的时间戳与代理时间戳差值超过随机延时）则给它解锁就可以解决单个代理的使用问题。同时，我们在所有代理之间轮询寻找没上锁的代理来使用，而每次请求的响应时间会积累下来，考虑到使用代理的请求响应时间比较长，顶多五六个代理之后旧代理就已经解锁了。而实际上，我的代理池维护的代理数通常不会少于6。即使代理全都上了锁，我也可以等待1秒后重新轮询。于是，只要保证代理池足够大，同时维护代理池中代理的质量，我们就可以实现连续发送请求，事实上，经过实验，代理响应时间受代理质量影响有所浮动，平均请求周期最长达到过6&#x2F;7秒，最短达到过5&#x2F;3秒。<br>&nbsp;&nbsp;&nbsp; 至于维护代理池的方法，我从爬虫进程fork了一个守护进程用来更新代理池。定时（暂定30秒）爬取代理网站某页面上的100个代理，然后用多线程（暂定16线程）测试每个代理的响应时间，过滤掉响应太慢（暂定超时时间为3秒）的代理，用同样的方法过滤代理池中不在新代理列表中的代理，最后合并代理池和新代理列表，同时为新代理打上很小的时间戳（实际上是0），已有的代理时间戳不变。这样可以在代理损失的过程中不断扩充代理，使代理池的规模稳定甚至递增（经过实验，基本能使代理池大小保持稳定，没有出现代理池暴增的情况，暂时不对代理池大小加以限制）。<br>&nbsp;&nbsp;&nbsp; 关于新思路的实现，我在重构weibo-crawler后新建了weibo-crawler2项目，按惯例已挂<a href=\"https://github.com/bipedalBit/weibo-crawler2\">git</a>。下面是一些weibo-crawler2的使用截图：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/weibo-crawler2/crawler2%E5%8C%85%E9%85%8D%E7%BD%AE.png\" alt=\"图１ crawler2包配置\" title=\"__init__.py\"><br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/weibo-crawler2/%E7%88%AC%E8%99%ABCLI%E8%BE%93%E5%87%BA.png\" alt=\"图２ 爬虫CLI输出\" title=\"爬虫CLI输出\"><br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/weibo-crawler2/%E7%88%AC%E8%99%AB%E6%97%A5%E5%BF%97.png\" alt=\"图３ 爬虫日志\" title=\"爬虫日志\"><br><strong>悲报，weibo.cn登录需要验证码了。刚开始验证码还比较弱，我正想着过完年要不要用深度学习破了它，然后验证码就变得更变态了…模拟登录模块报废。</strong></p>\n",
            "tags": [
                "python",
                "网络爬虫",
                "proxy"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2016/01/02/weibo-cn%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E5%99%A8%E7%9A%84python%E9%87%8D%E6%9E%84/",
            "url": "https://blog.bipedalbit.net/2016/01/02/weibo-cn%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E5%99%A8%E7%9A%84python%E9%87%8D%E6%9E%84/",
            "title": "weibo.cn模拟登录器的python重构",
            "date_published": "2016-01-02T04:47:05.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 之前封装过一个完整的微博爬虫工具包，但是在我查找python多进程爬虫的相关资料时发现，其实使用urllib2写网络爬虫已经是陈年往事，如今大家都用requests包，就像明明有.NET和Qt框架偏偏要用MFC框架，我是自找麻烦了。在使用requests包逐个重构网络爬虫部件的时候，对weibo.cn的模拟登录过程有了一些新的理解，于是来写篇文介绍一下。</p>\n<span id=\"more\"></span>\n<p>&nbsp;&nbsp;&nbsp; 我们再来重新分析一遍weibo.cn的登录过程。出发前先来检查一下装备：</p>\n<ul>\n<li>firefox浏览器</li>\n<li>一个firefox浏览器下的HTTP报文记录分析插件——httpfox</li>\n<li>python2语言环境——python 2.7</li>\n<li>一个python HTTP包——requests</li>\n<li>一个python XML解析包——BueatifulSoup4</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp; 妥，先来用httpfox记录一下手工登录的过程：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E5%BE%AE%E5%8D%9A%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E6%80%BB%E8%A7%88.png\" alt=\"图１ weibo.cn模拟登录过程总览\" title=\"weibo.cn模拟登录过程总览\"><br>&nbsp;&nbsp;&nbsp; 无视掉第二条失败的图片请求，剩下的就是weibo.cn登录的全过程。我们逐条分析：<br><img src=\"https://cdn.minio.bipedalbit.net/bipedalbit-net-images/%E5%BE%AE%E5%8D%9A%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/%E8%AF%B7%E6%B1%82%E7%99%BB%E5%BD%95%E9%A1%B5%E9%9D%A2.png\" alt=\"图２ 请求登录页面\" title=\"请求登录页面\"><br>&nbsp;&nbsp;&nbsp; 上图中是第一条请求的报文头部分，第一条请求是在访问weibo.cn的登录页面。请求到的页面源码中，掐头去尾，比较关键的部分如下：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs html\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">form</span> <span class=\"hljs-attr\">action</span>=<span class=\"hljs-string\">&quot;?backURL=http%3A%2F%2Fweibo.cn%2F<span class=\"hljs-symbol\">&amp;amp;</span>backTitle=%E5%BE%AE%E5%8D%9A<span class=\"hljs-symbol\">&amp;amp;</span>vt=1<span class=\"hljs-symbol\">&amp;amp;</span>revalid=2<span class=\"hljs-symbol\">&amp;amp;</span>ns=1&quot;</span> <span class=\"hljs-attr\">method</span>=<span class=\"hljs-string\">&quot;post&quot;</span>&gt;</span><br>\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">div</span>&gt;</span><br>\t\t手机号/电子邮箱/会员帐号:<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>/&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;text&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;mobile&quot;</span> <span class=\"hljs-attr\">size</span>=<span class=\"hljs-string\">&quot;30&quot;</span> <span class=\"hljs-attr\">value</span>=<span class=\"hljs-string\">&quot;&quot;</span> /&gt;</span><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>/&gt;</span><br>\t\t密码:<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>/&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;password&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;password_3523&quot;</span> <span class=\"hljs-attr\">size</span>=<span class=\"hljs-string\">&quot;30&quot;</span> /&gt;</span><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>/&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;checkbox&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;remember&quot;</span> <span class=\"hljs-attr\">checked</span>=<span class=\"hljs-string\">&quot;checked&quot;</span> /&gt;</span>记住登录状态，需支持并打开手机的cookie功能。<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">br</span>/&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;hidden&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;backURL&quot;</span> <span class=\"hljs-attr\">value</span>=<span class=\"hljs-string\">&quot;http%3A%2F%2Fweibo.cn%2F&quot;</span> /&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;hidden&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;backTitle&quot;</span> <span class=\"hljs-attr\">value</span>=<span class=\"hljs-string\">&quot;微博&quot;</span> /&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;hidden&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;tryCount&quot;</span> <span class=\"hljs-attr\">value</span>=<span class=\"hljs-string\">&quot;&quot;</span> /&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;hidden&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;vk&quot;</span> <span class=\"hljs-attr\">value</span>=<span class=\"hljs-string\">&quot;3523_4ea5_1803939589&quot;</span> /&gt;</span><br>\t\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">input</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;submit&quot;</span> <span class=\"hljs-attr\">name</span>=<span class=\"hljs-string\">&quot;submit&quot;</span> <span class=\"hljs-attr\">value</span>=<span class=\"hljs-string\">&quot;登录&quot;</span> /&gt;</span><br>\t<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">div</span>&gt;</span><br><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">form</span>&gt;</span><br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 可以看到表单的action地址、密码输入框的name、隐藏变量vk的值都是变动的，这三样东西正是我们在模拟登录时需要从登录页面解析获得的。对应的python代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 尽管不设置报文头也不会被新浪服务器阻拦，还是做一些必要的伪装与配置比较保险</span><br>headers = &#123;<br>\t<span class=\"hljs-string\">&#x27;User-Agent&#x27;</span>: <span class=\"hljs-string\">&#x27;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:42.0) Gecko/20100101 Firefox/42.0&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;DNT&#x27;</span>: <span class=\"hljs-string\">&#x27;1&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;Connection&#x27;</span>: <span class=\"hljs-string\">&#x27;keep-alive&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;Cache-Control&#x27;</span>: <span class=\"hljs-string\">&#x27;max-age=0&#x27;</span>,<br>&#125;<br><br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_post_needs</span>():<br>\turl = <span class=\"hljs-string\">&#x27;http://login.weibo.cn/login/?ns=1&amp;revalid=2&amp;backURL=http://weibo.cn/&amp;backTitle=微博&amp;vt=&#x27;</span><br>\ttext = requests.get(url, headers=headers).text<br>\tsoup = BeautifulSoup(text, <span class=\"hljs-string\">&#x27;lxml&#x27;</span>)<br>\taction_url = <span class=\"hljs-string\">&#x27;http://login.weibo.cn/login/&#x27;</span> + soup.form[<span class=\"hljs-string\">&#x27;action&#x27;</span>]<br>\tpassword_tag = soup.find(<span class=\"hljs-string\">&#x27;input&#x27;</span>, <span class=\"hljs-built_in\">type</span> = <span class=\"hljs-string\">&#x27;password&#x27;</span>)<br>\tpassword_name = re.search(<span class=\"hljs-string\">&#x27;(?&lt;=name=&quot;)\\w+(?=&quot;)&#x27;</span>, <span class=\"hljs-built_in\">str</span>(password_tag)).group(<span class=\"hljs-number\">0</span>)<br>\tvk_tag = soup.find(<span class=\"hljs-string\">&#x27;input&#x27;</span>, attrs = &#123;<span class=\"hljs-string\">&#x27;name&#x27;</span>: <span class=\"hljs-string\">&#x27;vk&#x27;</span>&#125;)<br>\tvk = re.search(<span class=\"hljs-string\">&#x27;(?&lt;=value=&quot;)\\w+(?=&quot;)&#x27;</span>, <span class=\"hljs-built_in\">str</span>(vk_tag)).group(<span class=\"hljs-number\">0</span>)<br>\t<span class=\"hljs-keyword\">return</span> action_url, password_name, vk<br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 页面解析完毕还需要填充表单，对应的python代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_post_data</span>(<span class=\"hljs-params\">username, password, password_name, vk</span>):<br>\tform_data = &#123;<br>\t\t<span class=\"hljs-string\">&#x27;mobile&#x27;</span>: username,<br>\t\tpassword_name: password,<br>\t\t<span class=\"hljs-string\">&#x27;remeber&#x27;</span>: <span class=\"hljs-string\">&#x27;on&#x27;</span>,<br>\t\t<span class=\"hljs-string\">&#x27;backURL&#x27;</span>: <span class=\"hljs-string\">&#x27;http://weibo.cn/&#x27;</span>,<br>\t\t<span class=\"hljs-string\">&#x27;backTitle&#x27;</span>: <span class=\"hljs-string\">&#x27;微博&#x27;</span>,<br>\t\t<span class=\"hljs-string\">&#x27;tryCount&#x27;</span>: <span class=\"hljs-string\">&#x27;&#x27;</span>,<br>\t\t<span class=\"hljs-string\">&#x27;vk&#x27;</span>: vk,<br>\t\t<span class=\"hljs-string\">&#x27;submit&#x27;</span>: <span class=\"hljs-string\">&#x27;登录&#x27;</span><br>\t&#125;<br>\t<span class=\"hljs-keyword\">return</span> form_data<br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 填充完登录表单，就该post出去了，由４个连续的状态码为302的请求不难看出，发送的post请求随后被进行了足足４次重定向。<br>&nbsp;&nbsp;&nbsp; 可以看到，第一次重定向后，请求访问了newlogin.sina.cn，此时新浪服务器已经给我们的请求加上了两个cookie字段（httpfox可以以字典的形式查看每个请求的cookie字段，这里我就不上图了），一个叫“_T_WM”另一个叫“SUB”，虽然不明白这两个字段的含义，但是经过实验得知，其实这个“SUB”字段就可以用来做登录验证了。<br>&nbsp;&nbsp;&nbsp; 第二次重定向请求访问了passport.weibo.com，这次新浪服务器给请求重新填充了许多cookie字段，但是仍然包含一个“SUB”字段，没错，这此的“SUB”也可以拿来做登录验证。<br>&nbsp;&nbsp;&nbsp; 类似的，第三次访问weibo.cn的重定向也填充了新cookie字段，我们又获得了一个可用的“SUB”字段。<br>&nbsp;&nbsp;&nbsp; 至于最后一次重定向，只是通过一个几乎纯JavaScript组成的页面让我们以已登录的状态跳转到前面表单参数中的backURL指向的页面。<br>&nbsp;&nbsp;&nbsp; 在使用python记录cookie字段时需要注意，这种经过若干次重定向的”长会话“不能直接使用Request对象来发送最初的post请求，而是需要一个能处理”长会话“的Session对象的帮助，获得cookie字段的对应python代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">wap_login</span>(<span class=\"hljs-params\">username, password</span>):<br>\turl, password_name, vk = get_post_needs()<br>\tdata = get_post_data(username, password, password_name, vk)<br>\tsession = requests.Session()<br>\tresponse = session.post(url, headers=headers, data=data)<br>\t<span class=\"hljs-comment\"># 以下三种来自不同domain的名为SUB的cookie字段都可以用来验证登录状态</span><br>\t<span class=\"hljs-comment\"># return session.cookies.get(&#x27;SUB&#x27;, domain=&#x27;.sina.cn&#x27;)</span><br>\t<span class=\"hljs-comment\"># return session.cookies.get(&#x27;SUB&#x27;, domain=&#x27;.weibo.com&#x27;)</span><br>\t<span class=\"hljs-keyword\">return</span> session.cookies.get(<span class=\"hljs-string\">&#x27;SUB&#x27;</span>, domain=<span class=\"hljs-string\">&#x27;.weibo.cn&#x27;</span>)<br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 拿到了必要的cookie字段，模拟登录过程其实就已经完成了，接下来我们也可以用下面的代码测试一下cookie字段是否管用：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">url = <span class=\"hljs-string\">&#x27;http://weibo.cn/moegirlwiki&#x27;</span><br>username = raw_input(<span class=\"hljs-string\">&#x27;请输入新浪通行证用户名：&#x27;</span>)<br>password = getpass(<span class=\"hljs-string\">&#x27;请输入新浪通行证密码：&#x27;</span>)<br>cookies = &#123;<span class=\"hljs-string\">&#x27;SUB&#x27;</span>: wap_login(username, password)&#125;<br>response = requests.get(url, cookies=cookies)<br><span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">&#x27;登录成功！&#x27;</span> <span class=\"hljs-keyword\">if</span> response.url == url <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">&#x27;登录失败！&#x27;</span><br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 如果读者接触过weibo.com的模拟登录过程想必明白，weibo.cn的登录过程简单多了，更重要的是，必要cookie字段在weibo.cn和weibo.com是通用的，那我们又何必选择更麻烦的模拟登录途径呢？</p>\n",
            "tags": [
                "python",
                "网络爬虫",
                "模拟登录"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2015/12/28/python%E7%BB%83%E6%89%8B%E4%B9%8B%E5%BE%AE%E5%8D%9A%E7%88%AC%E8%99%AB/",
            "url": "https://blog.bipedalbit.net/2015/12/28/python%E7%BB%83%E6%89%8B%E4%B9%8B%E5%BE%AE%E5%8D%9A%E7%88%AC%E8%99%AB/",
            "title": "python练手之微博爬虫",
            "date_published": "2015-12-28T13:53:49.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; 从半年前声称完成python入门以来，从来没有进行过非API调用的python实战，之前的BP神经网络python版也只是用了pybrain包提供的API而已。惊觉这样下去可能直到我把python语法忘干净都不敢说自己真的掌握了python，但是仍然无所事事拖到大概20天前，室友实验室发下任务要写个Java爬虫，我才决定同时写个python爬虫看看能不能体现下python开发效率的优势。</p>\n<span id=\"more\"></span>\n<h2 id=\"1-什么是网络爬虫\"><a href=\"#1-什么是网络爬虫\" class=\"headerlink\" title=\"1. 什么是网络爬虫\"></a>1. 什么是网络爬虫</h2><p>&nbsp;&nbsp;&nbsp; 在展开介绍我的python爬虫程序之前先来回顾一下网络爬虫实际上要做些什么事。<br>&nbsp;&nbsp;&nbsp; 从结果来看，网络爬虫是一种<strong>从互相关联的互联网网页上批量获取网页数据的技术手段</strong>。从流程上来看，爬虫程序是一个递归过程：</p>\n<ul>\n<li>（１）请求网页数据，开始步骤（２）；</li>\n<li>（２）解析网页数据，分别解析出<strong>目的数据</strong>和请求下一个网页的<strong>超链接地址</strong>；</li>\n<li>（３）如果不再有符合条件的<strong>超链接地址</strong>则结束递归过程，否则回到步骤（１）。</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp; 从图论的角度看，如果把网页看成搜索树的状态结点，那么常见的网络爬虫可以看成是一个单一分支的深度优先搜索，当然用于搜索引擎的网络爬虫则常常需要解析出网页中蕴含的多个搜索分支，这就更像我们熟悉的深度优先搜索了。<br>&nbsp;&nbsp;&nbsp; 怎么样，其实很简单吧。计算机技术中最通用的技术，其原理往往是最简单的。</p>\n<h2 id=\"2-为什么需要网络爬虫\"><a href=\"#2-为什么需要网络爬虫\" class=\"headerlink\" title=\"2.为什么需要网络爬虫\"></a>2.为什么需要网络爬虫</h2><p>&nbsp;&nbsp;&nbsp; 网络爬虫最常见的应用场景就是获取（遍历）多页数据列表中的所有数据，比如获取网易云音乐某个歌单中的所有音乐地址甚至音乐数据本身，又比如获取豆瓣电影某个主题下的所有电影。<br>&nbsp;&nbsp;&nbsp; 事实上，互联网时代到来之后，大数据时代接踵而至，计算机乃至其他领域的大量实验数据都来自互联网。除了直接从服务器获得数据之外，最普遍的数据获取途径就是通过最大的开放性数据来源WWW（世界范围Web）网络来请求数据了，毕竟比起HTTP协议，其他应用层通讯协议太多太杂太难分析，也不够开放。于是爬虫也成为了一种大量实验数据的获取手段。</p>\n<h2 id=\"3-怎么实现网络爬虫\"><a href=\"#3-怎么实现网络爬虫\" class=\"headerlink\" title=\"3. 怎么实现网络爬虫\"></a>3. 怎么实现网络爬虫</h2><p>&nbsp;&nbsp;&nbsp; 我选择了python作为写网络爬虫的语言，事实上据我所知，Java、PHP、C++、Ruby、Perl等我们熟知的绝大多数完善的编程语言都能用来写网络爬虫，或者说，只要有网络通信库的语言都可以用来写网络爬虫。其实相对来说，python的网络通信库还是很发达的，再加上python语言语法的简洁性和数据结构操作天生的灵活性，我觉得用python写网络爬虫没准是最快的。（别说Java可以用封装好的现成爬虫Jar包，没有可比性）<br>&nbsp;&nbsp;&nbsp; 我们通过实例来了解网络爬虫的实现过程。我选择了新浪微博作为爬虫对象，具体来说是将特定新浪微博用户的微博主页作为爬取对象。这里有两点需要注意：<br>&nbsp;&nbsp;&nbsp; 首先，我选择的是<a href=\"http://weibo.cn/\">微博移动版</a>的页面而非<a href=\"http://weibo.com/\">PC版</a>的页面作为爬取对象。移动版的页面结构非常简单而且不含任何JavaScript成分，微博数据相对很容易解析出来，而PC版的页面所有的微博数据都由JavaScript填充，虽然不至于找不到数据但是XML结构之外解析数据就几乎要全靠正则表达式了。此外虽然PC版的页面看起来蕴含更多信息，但是许多信息都是通过JavaScript，通过AJAX技术动态向服务器申请来的。如果选择PC版页面作为爬取对象，将大量增加页面解析难度，甚至可能需要让爬虫过程发生嵌套，使整个爬虫流程十分复杂。如果数据能从移动版页面获取，尽量不要选择解析PC版页面。<br>&nbsp;&nbsp;&nbsp; 然后，需要想办法获得登录微博后的cookie，或者cookie的必要字段。微博跟早期的百度贴吧不一样，不登录微博账号的话什么微博数据都看不到。平时我们的浏览器如果没有保存微博登录后的cookie，在访问包含微博数据的页面地址时，HTTP请求会被重定向到登录页面。只有在访问页面时令HTTP请求报文头中携带登录后的cookie，该请求才被服务器认定为合法请求，才会正常的返回被请求的页面。获取cookie有两种途径：</p>\n<ul>\n<li>通过各种浏览器的HTTP报文抓取插件，从HTTP请求的文件头中提取出现成的cookie；</li>\n<li>模拟微博账号登录过程，在登录完成之后的第一次页面访问请求的HTTP报文头中提取出cookie。</li>\n</ul>\n<h3 id=\"3-1-python下的HTTP请求\"><a href=\"#3-1-python下的HTTP请求\" class=\"headerlink\" title=\"3.1 python下的HTTP请求\"></a>3.1 python下的HTTP请求</h3><p>&nbsp;&nbsp;&nbsp; python下如何完成一次HTTP请求并获得返回的页面数据？哦顺便一说我用的是python2，python3应该区别不大。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">import</span> urllib2<br><span class=\"hljs-built_in\">print</span> urllib2.urlopen(<span class=\"hljs-string\">&#x27;www.baidu.com&#x27;</span>).read()<br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 完了？完了。是不是很简单？python大法好！开个玩笑不要在意，上面的两行代码只是实现了最简单的默认请求形式及返回数据的获取。实际上urllib2类的urlopen方法参数可以是一个url或者是一个封装好的Request对象，返回值是一个封装好的Response对象。python的网络编程确实很简单直观，我不想细说，至于urllib2的具体的各种相关python API，请自行查阅<a href=\"https://docs.python.org/2/library/urllib2.html\">手册</a>或者等下看我的源码。</p>\n<h3 id=\"3-2-页面解析\"><a href=\"#3-2-页面解析\" class=\"headerlink\" title=\"3.2 页面解析\"></a>3.2 页面解析</h3><p>&nbsp;&nbsp;&nbsp; 页面解析是爬取页面数据后的后续操作，我们实际需要的数据往往蕴含在页面源码之中，最简单粗暴的解析方式就是观察页面源码然后用正则表达式匹配出需要的数据字段。此外既然页面源码的组织方式是HTML，而HTML是一种XML结构，那么我们就可以用一些XML解析工具比如BueatifulSoup、XPath来辅助页面解析。这些工具比较容易寻找到整个HTML文本数据中需要的标签，接下来再用正则表达式简单处理一下，数据就不难获得了。顺便一提，python的正则表达式包叫做re，用法参见<a href=\"https://docs.python.org/2/library/re.html\">手册</a>。</p>\n<h3 id=\"3-3-模拟登录\"><a href=\"#3-3-模拟登录\" class=\"headerlink\" title=\"3.3 模拟登录\"></a>3.3 模拟登录</h3><p>&nbsp;&nbsp;&nbsp; 固然，我们可以偷懒一直选择填入现成的cookie，但是须知，cookie都是有时限或者说寿命的，一定时间后，cookie会过期失效，不再合法。这时就需要重新登录获得一个新的cookie了。虽然cookie过期并没有那么快，但是总是需要有人在侧监视，随时更换cookie来维持比较长的爬虫过程，岂不是显得很low很不自动化？所以大部分比较大、比较正式的爬虫程序都会选择实现特定网站的模拟登录过程。<br>&nbsp;&nbsp;&nbsp; 具体到微博上来，weibo.com即PC版微博网站的登录过程网上有相当多的分析文章，比如<a href=\"http://www.douban.com/note/201767245/\">这篇</a>、<a href=\"http://www.cnblogs.com/myx/archive/2011/10/19/Sina-SSO.html\">这篇</a>和<a href=\"http://www.360doc.com/content/15/0514/19/13228794_470491590.shtml\">这篇</a>。读者需要注意的是，据我所知weibo.com的登录过程微调十分频繁，写一个weibo.com模拟登录可能几个月后就需要根据新浪的微调调整代码了。至于weibo.cn，网上简单的找找似乎没有发现分析文章。但是当时据我猜测，weibo.cn的登录过程应该比weibo.com简单（事实证明确实如此），登录过程微调后修改代码也会更容易。而登录后的cookie，经过我的实验，是通用的。也就是说不论在weibo.cn还是weibo.com，登录后产生的细节各不相同的cookie可能有一个共同部分是用来验证登录状态的（事实再次证明确实如此）。<br>&nbsp;&nbsp;&nbsp; 我使用的HTTP报文抓取插件是firefox下的HttpFox，同类插件还有httpwatch、firefox的自带工具、chrome的自带工具，甚至一些较新版本的IE自带工具。记录登录过程，分析跳转了哪些地址，有几次重定向，期间请求携带了哪些数据，cookie有什么变化。这里我简单说下weibo.cn的登录过程。<br>&nbsp;&nbsp;&nbsp; 总体来说weibo.cn的登录过程分成三个步骤，请求登录页面，获取一些post登录信息的必要参数；post登录信息表单；等待。<br>&nbsp;&nbsp;&nbsp; 登录页面中的action（post目标地址）、加后缀的密码数据键名（如password_2358）和一个叫做vk的数据都是提交登录表单时必要的。<br>&nbsp;&nbsp;&nbsp; 之后就要把表单加工成为可填入Request对象的形式，代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">form_data = &#123;<br>\t<span class=\"hljs-string\">&#x27;mobile&#x27;</span>: username,<br>\tpassword_name: password,<br>\t<span class=\"hljs-string\">&#x27;remeber&#x27;</span>: <span class=\"hljs-string\">&#x27;on&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;backURL&#x27;</span>: <span class=\"hljs-string\">&#x27;http://weibo.cn/&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;backTitle&#x27;</span>: <span class=\"hljs-string\">&#x27;微博&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;tryCount&#x27;</span>: <span class=\"hljs-string\">&#x27;&#x27;</span>,<br>\t<span class=\"hljs-string\">&#x27;vk&#x27;</span>: vk,<br>\t<span class=\"hljs-string\">&#x27;submit&#x27;</span>: <span class=\"hljs-string\">&#x27;登录&#x27;</span><br>&#125;<br>form_data = urllib.urlencode(form_data)<br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 提交表单后新浪服务器会对这一请求进行４次重定向，注意每次重定向都需要提取并重填充cookie字段，因为python的默认重定向处理类不会重填cookie，重写的重定向处理方法如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyRedirectHandler</span>(urllib2.HTTPRedirectHandler):<br>\t<span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">\t继承urllib2.HTTPRedirectHandler类，封装http_error_302方法。</span><br><span class=\"hljs-string\">\t在自动执行重定向发送新的HTTP请求前提取附在请求头中的cookie，</span><br><span class=\"hljs-string\">\t提取cookie中模拟登录必需的字段并存储在类成员变量中。</span><br><span class=\"hljs-string\">\t&#x27;&#x27;&#x27;</span><br>\t<span class=\"hljs-comment\"># 类成员变量，存储以登录状态访问新浪微博必需的cookie字段</span><br>\tcookie = <span class=\"hljs-string\">&#x27;&#x27;</span><br><br>\t<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">http_error_302</span>(<span class=\"hljs-params\">self, req, fp, code, msg, headers</span>):<br>\t\t<span class=\"hljs-string\">&#x27;&#x27;&#x27;</span><br><span class=\"hljs-string\">\t\t添加cookie处理过程，然后调用原http_error_302方法执行自动跳转。</span><br><span class=\"hljs-string\">\t\t&#x27;&#x27;&#x27;</span><br>\t\tcookie = <span class=\"hljs-built_in\">str</span>(headers[<span class=\"hljs-string\">&quot;Set-Cookie&quot;</span>])<br>\t\t<span class=\"hljs-keyword\">if</span> re.search(<span class=\"hljs-string\">&#x27;SUB=.+?;&#x27;</span>, cookie) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>\t\t\tMyRedirectHandler.cookie = re.search(<span class=\"hljs-string\">&#x27;SUB=.+?(?=;)&#x27;</span>, cookie).group(<span class=\"hljs-number\">0</span>)<br>\t\treq.add_header(<span class=\"hljs-string\">&quot;Cookie&quot;</span>, cookie)<br>\t\t<span class=\"hljs-keyword\">return</span> urllib2.HTTPRedirectHandler.http_error_302(<span class=\"hljs-variable language_\">self</span>, req, fp, code, msg, headers)<br></code></pre></td></tr></table></figure>\n<p>&nbsp;&nbsp;&nbsp; 最后一次重定向几乎是一个纯JavaScript页面，做的事情是通过JavaScript跳转向一个地址。实验证明最后这一次跳转对于登录过程并没有什么意义，看起来是故弄玄虚。记得我们刚才重写的重定向处理方法吗？我们在重不断重填cookie的过程中也记录了cookie某个字段的更新情况，最后，我们手里有那个字段的最新值。这个cookie字段正是微博登录验证唯一必需的cookie字段。至于我怎么知道是这个字段，对照实验。<br>&nbsp;&nbsp;&nbsp; 是不是比weibo.com的登录过程简单多了？</p>\n<h3 id=\"3-4-可能的改进\"><a href=\"#3-4-可能的改进\" class=\"headerlink\" title=\"3.4 可能的改进\"></a>3.4 可能的改进</h3><p>&nbsp;&nbsp;&nbsp; 读者可能发觉了，每次请求的间隔长达几秒，这个爬虫程序与其说是一个高效获取数据的工具，不如说是一个模拟人类操作的脚本而已。简单概括一下就是，爬取页面太慢了。那么如何改进呢？并行化。模拟一个人的访问太慢，那么模拟更多人的访问就好。<br>&nbsp;&nbsp;&nbsp; 并行改进有几个层次，硬件架构的并行化（多核，流水线，长指令等等）、多线程、多进程和分布式。硬件层次的改进对我们这个爬虫程序意义不大，多线程在别的语言中当然可行，可是据我所知，python中的多线程性能收到了限制，python中应该尽量使用多进程。在多进程的基础上，实现完善的分布式通信，就能做分布式并行了。<br>&nbsp;&nbsp;&nbsp; 有读者可能会想到，直接把程序改成多进程的难道不是相当于加大了请求频率而已？是的，确实是这样，所以进程间应该“完全独立”，一个进程被新浪服务器封禁，不应该影响其他进程。这里就涉及一个问题，新浪服务器封禁的是微博账号还是IP地址呢？经过实验答案是IP地址，实际上，目前大多数服务器的反DDoS机制就是封IP。这就好办了，微博账号申请还相对麻烦，可是免费或收费的IP代理却很容易批量获得。比如<a href=\"http://www.xicidaili.com/\">这里</a>，其中免费的可以直接爬取，肯花钱就能直接获得官方批量代理的接口。当然，广域网分布式具有天然的互异IP，也可以选择使用多台机器，不使用代理。</p>\n<p>&nbsp;&nbsp;&nbsp; 全文基本没讲解代码是不是有点过分？这次确实有点偷懒，但是源码里注释写的还是挺详细了。上干货，我把完成的单进程python微博爬虫简单封装了一下，挂在了<a href=\"https://github.com/bipedalBit/weibo-crawler\">自家git仓库</a>。</p>\n",
            "tags": [
                "python",
                "网络爬虫"
            ]
        },
        {
            "id": "https://blog.bipedalbit.net/2015/10/13/python%E7%9A%84SimpleHTTPServer/",
            "url": "https://blog.bipedalbit.net/2015/10/13/python%E7%9A%84SimpleHTTPServer/",
            "title": "python的SimpleHTTPServer",
            "date_published": "2015-10-13T09:46:25.000Z",
            "content_html": "<p>&nbsp;&nbsp;&nbsp; Linux下想要给局域网的其他终端共享文件大家一般会怎么做？是开个Nginx服务器或者Apache服务器？还是装个开源的FTP软件？其实还有更便捷的原生办法。</p>\n<span id=\"more\"></span>\n<p>&nbsp;&nbsp;&nbsp; Linux一般都预装了python，有个神奇的python命令：<br>&nbsp;&nbsp;&nbsp; <code>python -m SimpleHTTPServer 80</code><br>&nbsp;&nbsp;&nbsp; 读者可以试试发生了什么，没错，正如命令的字面意思，本地80口被开了个简易的HTTP服务器。如果当前文件夹下没有主页，局域网终端就可以通过本地IP访问本机并看到到执行该命令的当前文件夹下的文件目录了，接下来只要通过浏览器就可以下载本机的文件。<br>&nbsp;&nbsp;&nbsp; 不知道读者准备怎么使用这个命令，我有时候会忘记这个命令的一些细节所以选择把这个命令写成一个shell脚本，需要共享哪个文件夹就把脚本拖到哪里运行一下。<br>&nbsp;&nbsp;&nbsp; 如果想看这个python命令更详细的介绍可以参阅<a href=\"https://docs.python.org/2/library/simplehttpserver.html\">python官方文档</a>。<br>&nbsp;&nbsp;&nbsp; 我的脚本也挂出来吧，应该只在debian系统下好用，仅供参考：</p>\n<pre><code class=\"hljs\">#!/bin/bash\nroute=`route|grep default`\nroute=$&#123;route##*&#39; &#39;&#125;\nif [[ &quot;$route&quot; == usb* ]]; then\n    echo 正在使用usb共享网络。\nelif [[ &quot;$route&quot; == wlan* ]]; then\n    echo 正在使用无线网络。\nelif [[ &quot;$route&quot; == eth* ]]; then\n    echo 正在使用有线网络。\nelse\n    echo 正在使用其他网络。\nfi\nip=`ifconfig $route|grep &#39;inet &#39;|cut -d&#39;:&#39; -f2|cut -d&#39; &#39; -f1`\necho 局域网ip地址：$ip\necho 现在你可以从其他终端通过地址 http://$ip:2333/ 访问当前目录。\npython -m SimpleHTTPServer 2333\n</code></pre>\n",
            "tags": [
                "python"
            ]
        }
    ]
}